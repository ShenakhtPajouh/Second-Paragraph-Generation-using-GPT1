{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"no_sgmnt_train.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mqXHWIaZgwmf","colab_type":"code","outputId":"9affaa27-4d3d-4843-fbe3-739d671e6d82","executionInfo":{"status":"ok","timestamp":1562345341047,"user_tz":-270,"elapsed":24167,"user":{"displayName":"Mehrnaz mofakhami","photoUrl":"","userId":"16321446277559839486"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"41eLJ3OegzRE","colab_type":"code","outputId":"c4a29282-ff93-4fcf-c730-d1629d87a5c2","executionInfo":{"status":"ok","timestamp":1562345341052,"user_tz":-270,"elapsed":23906,"user":{"displayName":"Mehrnaz mofakhami","photoUrl":"","userId":"16321446277559839486"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%cd '/content/drive/My Drive/shared-works/Thread-1'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/shared-works/Thread-1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"02gueGDWg7Pl","colab_type":"code","outputId":"05052540-b310-4933-939f-82a4e91f74f6","executionInfo":{"status":"ok","timestamp":1562345347883,"user_tz":-270,"elapsed":27426,"user":{"displayName":"Mehrnaz mofakhami","photoUrl":"","userId":"16321446277559839486"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["!pip install ftfy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0Ur_ssMgt-u","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import pickle\n","from model import Transformer\n","from utils import iter_data, Logger\n","import time\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gSRsBWTMg9Ar","colab_type":"code","colab":{}},"source":["def save(model, train_loss_results, validation_loss_results, cnt):\n","    with open('./Data/interpretation/no_segment/pickle/train_loss_results.pkl', 'wb') as pkl:\n","        pickle.dump(train_loss_results, pkl)\n","\n","    with open('./Data/interpretation/no_segment/pickle/validation_loss_results.pkl', 'wb') as pkl:\n","        pickle.dump(validation_loss_results, pkl)\n","\n","    model.save_weights(\"./checkpoints/interpretation/no_segment/model/cp-{}.ckpt\".format(format(cnt)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghLZgCQzg_gc","colab_type":"code","colab":{}},"source":["def format(x, max_len=4):\n","    x = str(x)\n","    return \"0\" * (max_len - len(x)) + x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U20F0EYYhB2M","colab_type":"code","colab":{}},"source":["def STLR(t, cut_frac = 0.1, ratio = 32, lr_max = 0.002, T = 1400000):\n","    cut = int(T * cut_frac)\n","    p = t / cut if t < cut else 1 - (t - cut) / (cut * (ratio - 1))\n","    lr = lr_max * (1 + p * (ratio - 1)) / ratio\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"navYdbCQhDx6","colab_type":"code","colab":{}},"source":["def decay(lr, t):\n","    lr -= lr * (1 / (t ** 0.5))\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6kbQNf_1cAV","colab_type":"code","colab":{}},"source":["def train(model, val_data_len, learning_rate=0.0005, \n","          n_epochs=100, n_embd = 768, n_vocab = 40478, n_batch=64, n_ctx=512, n_special = 1, n_segment = 3,\n","          train_steps=100, validation_steps=5000, save_steps=5000, log_path='train.log', lr_fn = 'decay', cnt=0):\n","    \n","    \"\"\"\n","        X : (batch size, seq len, 3 (IDs and positions and segments)) -> tokens\n","        M1: (batch size, seq len) -> masks for getting 2nd paragraph in input tokens\n","        M2: (batch size, seq len) -> masks for getting 2nd paragraph in predicted tokens\n","    \"\"\"\n","    \n","    \n","    def parse_function(data_record):\n","        features={\n","            'triple': tf.VarLenFeature(tf.int64),\n","            'tokens_mask': tf.VarLenFeature(tf.int64),\n","            'preds_mask': tf.VarLenFeature(tf.int64),\n","            'book_id': tf.FixedLenFeature([], tf.int64),\n","            'counter': tf.FixedLenFeature([], tf.int64),\n","            'file_id': tf.FixedLenFeature([], tf.int64)\n","        }\n","        \n","        example=tf.parse_single_example(data_record, features)\n","        tp=example['triple'].values\n","        tp=tp-1\n","        tm=example['tokens_mask'].values\n","        tm=tm-1\n","        pm=example['preds_mask'].values\n","        pm=pm-1\n","        md=(example['book_id'], example['counter'], example['file_id'])\n","        print(\".............\")\n","        print(type(tp), type(tm), type(pm), type(md))\n","        \n","        return (tp, tm, pm, md[0], md[1], md[2])\n","        \n","    def create_dataset(n_files, n_batch, train=True):\n","        tfrecord_files=[]\n","        if train:\n","            for b in range(n_files-1):\n","                f_name='/content/drive/My Drive/shared-works/Thread-1/Data/interpretation/no_segment/'+str(b)+'.tfrecord'\n","                tfrecord_files.append(f_name)\n","                \n","        else:\n","            f_name='/content/drive/My Drive/shared-works/Thread-1/Data/interpretation/no_segment/'+str(n_files-1)+'.tfrecord'\n","            tfrecord_files.append(f_name)\n","\n","        dataset=tf.data.TFRecordDataset(tfrecord_files)\n","        dataset=dataset.map(parse_function)\n","        dataset=dataset.shuffle(buffer_size=10000)\n","        dataset=dataset.padded_batch(batch_size=n_batch, padded_shapes=((None,),(None,),(None,),(),(),()), drop_remainder=True)\n","        dataset=dataset.repeat()\n","        return dataset\n","    \n","    tf.reset_default_graph()\n","    \n","    t_dataset=create_dataset(7, n_batch)\n","    t_iterator=t_dataset.make_one_shot_iterator()\n","    X_t,M1_t,M2_t,md_t0, md_t1, md_t2=t_iterator.get_next()\n","    \n","    v_dataset=create_dataset(7, n_batch, train=False)\n","    v_iterator=v_dataset.make_one_shot_iterator()\n","    X_v,M1_v,M2_v,md_v0, md_v1, md_v2=v_iterator.get_next()\n","    \n","    t_saveable = tf.data.experimental.make_saveable_from_iterator(t_iterator)\n","    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, t_saveable)\n","    \n","    v_saveable = tf.data.experimental.make_saveable_from_iterator(v_iterator)\n","    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, v_saveable)\n","        \n","        \n","\n","    \n","#     tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, model.variables)\n","        \n","    X = tf.placeholder(tf.int32, [None, None, 2])\n","    M1 = tf.placeholder(tf.int32, [None, None])\n","    M2 = tf.placeholder(tf.int32, [None, None])\n","    logits, losses = model([X, M1, M2])\n","    # Create model's graph\n","\n","    \n","\n","    lr = tf.placeholder(tf.float32, shape=[])\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n","    # Define Optimizer\n","    \n","    \n","    real_grads = tf.gradients(losses, model.variables)\n","    real_grads[0] = tf.convert_to_tensor(real_grads[0])\n","#     fake_grads = [tf.zeros_like(grad, dtype = tf.float32) for grad in real_grads]\n","    \n","#     def set_zero():\n","#         return real_grads\n","    \n","#     def preserve():\n","#         return fake_grads\n","    \n","#     limit = tf.constant(5.64)\n","#     grads = tf.cond(tf.greater(losses, limit), set_zero, preserve)\n","    grads_and_vars = zip(real_grads, model.variables)\n","    capped_grads_and_vars = [(tf.clip_by_norm(grad, 0.25), var) for grad, var in grads_and_vars]\n","    train_op = optimizer.apply_gradients(capped_grads_and_vars)\n","    # Create nodes for applying gradients\n","    \n","\n","    print(tf.get_collection(tf.GraphKeys.SAVEABLE_OBJECTS))\n","    print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n","    \n","    \n","    sess = tf.Session()\n","    tf.keras.backend.set_session(sess)\n","    sess.run(tf.global_variables_initializer())\n","    \n","    saver=tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.SAVEABLE_OBJECTS), max_to_keep=0)\n","    \n","    if cnt==0:\n","        \n","        model.load_weights('./checkpoints/Segment/cp-0001.ckpt')\n","        train_loss_results=[]\n","        validation_loss_results=[]\n","    \n","    if cnt>0:\n","        \n","#         tf.reset_default_graph()\n","#         imported_graph = tf.train.import_meta_graph('./checkpoints/language_model/my-model0.meta')\n","#         imported_graph.restore(sess, './checkpoints/language_model/my-model0')\n","        \n","        \n","        print('before')\n","        saver.restore(sess, \"./checkpoints/interpretation/no_segment/iterator/svble-{}\".format(format(cnt)))\n","        print('after')\n","        load_path=\"./checkpoints/interpretation/no_segment/model/cp-{}.ckpt\".format(format(cnt))\n","        model.load_weights(load_path)\n","        with open('./Data/interpretation/no_segment/pickle/train_loss_results.pkl', 'rb') as pkl:\n","            train_loss_results = pickle.load(pkl)\n","        with open('./Data/interpretation/no_segment/pickle/validation_loss_results.pkl', 'rb') as pkl:\n","            validation_loss_results = pickle.load(pkl)\n","    \n","\n","    train_losses = []\n","    \n","\n","    \n","\n","    step=50000\n","#     step = 510000\n","#     cnt = 85\n","    max_loss = 0\n","#     train_generator = iter_data(n_batch, n_epochs)\n","    logger = Logger(path=log_path)\n","    start = time.time()\n","    \n","    np.set_printoptions(precision = 4)\n","\n","    while True:\n","        step += 1\n","        if step%100==0:\n","            print('step:',step)\n","        \n","        tokenss, maskss1, maskss2, m1, m2, m3=sess.run([X_t,M1_t,M2_t,md_t0, md_t1, md_t2])\n","        tokens=np.asarray(tokenss).reshape([n_batch,-1,3])[:,:,:2]\n","        masks1=np.asarray(maskss1)\n","        masks2=np.asarray(maskss2)\n","#         print('shapes')\n","#         print(tokens)\n","#         print(masks1)\n","#         print(masks2)\n","#         print(tokens.shape)\n","#         print(masks1.shape)\n","#         print(masks2.shape)\n","        metadata=(m1,m2,m3)\n","        if lr_fn == 'STLR':\n","            _, train_loss, gradients = sess.run([train_op, losses, real_grads], {X: tokens, M1: masks1, M2: masks2, lr: STLR(step)})\n","\n","        else:\n","            _, train_loss, gradients = sess.run([train_op, losses, real_grads], {X: tokens, M1: masks1, M2: masks2, lr: learning_rate})   \n","            \n","\n","        if train_loss > max_loss:\n","            max_loss = train_loss\n","            print('\\n {} {} , {}\\n'.format(step, metadata, np.power(2, train_loss)))\n","            \n","        else:\n","            train_losses.append(train_loss)\n","            \n","        if step % 1000 == 1:\n","            print('\\n' + str(np.linalg.norm(gradients[3])) + \"  \" + str(np.linalg.norm(gradients[0])) + '\\n')\n","        \n","        if step % train_steps == 0:\n","            print('learning_rate', learning_rate)\n","            if lr_fn != 'STLR':\n","                learning_rate = decay(learning_rate, step)\n","\n","            train_loss_results.append(sum(train_losses) / len(train_losses))\n","            train_losses = []\n","            logger.log(step=step, train_loss=train_loss_results[-1], time=time.time() - start)\n","            print('Step: {} -- Time: {} => ppl: {}'.format(step, int(time.time() - start), np.power(2, train_loss_results[-1])))\n","\n","        if step % validation_steps == 0:\n","            print('validation')\n","#             validation_generator = iter_data(n_batch, train=False)\n","            validation_losses = []\n","            for rr in range(val_data_len):\n","                validation_tokenss, validation_maskss1, validation_maskss2, _,_,_=sess.run([X_v,M1_v,M2_v,md_v0, md_v1, md_v2])\n","                validation_tokens=np.asarray(validation_tokenss).reshape([n_batch,-1,3])[:,:,:2]\n","                validation_masks1=np.asarray(validation_maskss1)\n","                validation_masks2=np.asarray(validation_maskss2)\n","                validation_losses.append(sess.run(losses, {X: validation_tokens, M1: validation_masks1, M2: validation_masks2}))\n","\n","            validation_loss_results.append(sum(validation_losses) / len(validation_losses))\n","            logger.log(step=step, validation_loss=validation_loss_results[-1], time=time.time() - start)\n","            print('### Step: {} -- Time: {} => ppl: {}'.format(step, int(time.time() - start), np.power(2, validation_loss_results[-1])))\n","\n","        if step % save_steps == 0:\n","            cnt += 1\n","            print('cnt',cnt)\n","            save(model, train_loss_results, validation_loss_results, cnt)\n","            save_path=saver.save(sess, \"./checkpoints/interpretation/no_segment/iterator/svble-{}\".format(format(cnt)))\n","                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2Ck4v1DhKso","colab_type":"code","colab":{}},"source":["model = Transformer(\"Model\", 40478)\n","val_data_len=500\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8josLpOOU7o","colab_type":"code","outputId":"60e46712-c187-4936-928e-a9170c620f6d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(model, val_data_len,n_batch = 2, n_epochs = 1, cnt=134)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0705 16:49:12.443856 139745439750016 deprecation.py:323] From <ipython-input-9-76d1df64fe1d>:56: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"],"name":"stderr"},{"output_type":"stream","text":[".............\n","<class 'tensorflow.python.framework.ops.Tensor'> <class 'tensorflow.python.framework.ops.Tensor'> <class 'tensorflow.python.framework.ops.Tensor'> <class 'tuple'>\n",".............\n","<class 'tensorflow.python.framework.ops.Tensor'> <class 'tensorflow.python.framework.ops.Tensor'> <class 'tensorflow.python.framework.ops.Tensor'> <class 'tuple'>\n"],"name":"stdout"},{"output_type":"stream","text":["W0705 16:49:13.502712 139745439750016 deprecation_wrapper.py:119] From /content/drive/My Drive/shared-works/Thread-1/model.py:55: The name tf.keras.initializers.random_normal is deprecated. Please use tf.compat.v1.keras.initializers.random_normal instead.\n","\n","W0705 16:49:13.505037 139745439750016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0705 16:49:13.907469 139745439750016 deprecation_wrapper.py:119] From /content/drive/My Drive/shared-works/Thread-1/model.py:124: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n","\n","W0705 16:49:14.028852 139745439750016 deprecation_wrapper.py:119] From /content/drive/My Drive/shared-works/Thread-1/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n","\n","W0705 16:49:19.349253 139745439750016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0705 16:49:19.397606 139745439750016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:253: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["[<tensorflow.python.data.experimental.ops.iterator_ops._Saveable object at 0x7f181b73b940>, <tensorflow.python.data.experimental.ops.iterator_ops._Saveable object at 0x7f181b73b9b0>]\n","[<tf.Variable 'Model/embedding/we:0' shape=(40994, 768) dtype=float32>, <tf.Variable 'Model/h//attn/conv1d/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h//attn/conv1d/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h//attn/conv1d_1/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h//attn/conv1d_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h//mlp/conv1d_2/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h//mlp/conv1d_2/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h//mlp/conv1d_3/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h//mlp/conv1d_3/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_1//attn/conv1d_4/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_1//attn/conv1d_4/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_1//attn/conv1d_5/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_1//attn/conv1d_5/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_1//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_1//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_1//mlp/conv1d_6/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_1//mlp/conv1d_6/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_1//mlp/conv1d_7/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_1//mlp/conv1d_7/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_1//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_1//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_2//attn/conv1d_8/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_2//attn/conv1d_8/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_2//attn/conv1d_9/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_2//attn/conv1d_9/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_2//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_2//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_2//mlp/conv1d_10/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_2//mlp/conv1d_10/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_2//mlp/conv1d_11/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_2//mlp/conv1d_11/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_2//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_2//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_3//attn/conv1d_12/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_3//attn/conv1d_12/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_3//attn/conv1d_13/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_3//attn/conv1d_13/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_3//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_3//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_3//mlp/conv1d_14/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_3//mlp/conv1d_14/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_3//mlp/conv1d_15/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_3//mlp/conv1d_15/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_3//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_3//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_4//attn/conv1d_16/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_4//attn/conv1d_16/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_4//attn/conv1d_17/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_4//attn/conv1d_17/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_4//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_4//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_4//mlp/conv1d_18/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_4//mlp/conv1d_18/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_4//mlp/conv1d_19/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_4//mlp/conv1d_19/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_4//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_4//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_5//attn/conv1d_20/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_5//attn/conv1d_20/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_5//attn/conv1d_21/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_5//attn/conv1d_21/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_5//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_5//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_5//mlp/conv1d_22/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_5//mlp/conv1d_22/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_5//mlp/conv1d_23/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_5//mlp/conv1d_23/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_5//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_5//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_6//attn/conv1d_24/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_6//attn/conv1d_24/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_6//attn/conv1d_25/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_6//attn/conv1d_25/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_6//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_6//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_6//mlp/conv1d_26/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_6//mlp/conv1d_26/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_6//mlp/conv1d_27/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_6//mlp/conv1d_27/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_6//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_6//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_7//attn/conv1d_28/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_7//attn/conv1d_28/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_7//attn/conv1d_29/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_7//attn/conv1d_29/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_7//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_7//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_7//mlp/conv1d_30/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_7//mlp/conv1d_30/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_7//mlp/conv1d_31/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_7//mlp/conv1d_31/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_7//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_7//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_8//attn/conv1d_32/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_8//attn/conv1d_32/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_8//attn/conv1d_33/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_8//attn/conv1d_33/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_8//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_8//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_8//mlp/conv1d_34/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_8//mlp/conv1d_34/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_8//mlp/conv1d_35/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_8//mlp/conv1d_35/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_8//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_8//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_9//attn/conv1d_36/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_9//attn/conv1d_36/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_9//attn/conv1d_37/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_9//attn/conv1d_37/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_9//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_9//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_9//mlp/conv1d_38/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_9//mlp/conv1d_38/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_9//mlp/conv1d_39/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_9//mlp/conv1d_39/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_9//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_9//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_10//attn/conv1d_40/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_10//attn/conv1d_40/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_10//attn/conv1d_41/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_10//attn/conv1d_41/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_10//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_10//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_10//mlp/conv1d_42/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_10//mlp/conv1d_42/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_10//mlp/conv1d_43/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_10//mlp/conv1d_43/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_10//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_10//ln_2/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_11//attn/conv1d_44/w:0' shape=(1, 768, 2304) dtype=float32>, <tf.Variable 'Model/h_11//attn/conv1d_44/b:0' shape=(2304,) dtype=float32>, <tf.Variable 'Model/h_11//attn/conv1d_45/w:0' shape=(1, 768, 768) dtype=float32>, <tf.Variable 'Model/h_11//attn/conv1d_45/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_11//ln_1/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_11//ln_1/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_11//mlp/conv1d_46/w:0' shape=(1, 768, 3072) dtype=float32>, <tf.Variable 'Model/h_11//mlp/conv1d_46/b:0' shape=(3072,) dtype=float32>, <tf.Variable 'Model/h_11//mlp/conv1d_47/w:0' shape=(1, 3072, 768) dtype=float32>, <tf.Variable 'Model/h_11//mlp/conv1d_47/b:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_11//ln_2/g:0' shape=(768,) dtype=float32>, <tf.Variable 'Model/h_11//ln_2/b:0' shape=(768,) dtype=float32>]\n"],"name":"stdout"},{"output_type":"stream","text":["W0705 16:49:29.564613 139745439750016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["before\n","after\n","\n"," 50001 (array([ 3760, 20154]), array([2596,  548]), array([2, 2])) , 16.16676210769255\n","\n","\n","0.7798813  1.5798302\n","\n","\n"," 50005 (array([20133, 19977]), array([ 976, 1057]), array([2, 2])) , 17.397327763308642\n","\n","\n"," 50034 (array([20154, 11961]), array([ 788, 1310]), array([2, 2])) , 32.712881036685125\n","\n","step: 50100\n","learning_rate 0.0005\n","Step: 50100 -- Time: 37 => ppl: 10.167530071233209\n","step: 50200\n","learning_rate 0.0004977661647419562\n","Step: 50200 -- Time: 67 => ppl: 9.884241661767371\n","step: 50300\n","learning_rate 0.000495544525623255\n","Step: 50300 -- Time: 93 => ppl: 10.366006816949835\n","step: 50400\n","learning_rate 0.0004933350017912679\n","Step: 50400 -- Time: 121 => ppl: 10.87372579008743\n","step: 50500\n","learning_rate 0.0004911375130369746\n","Step: 50500 -- Time: 147 => ppl: 9.8951308788685\n","step: 50600\n","learning_rate 0.0004889519797888957\n","Step: 50600 -- Time: 174 => ppl: 10.30198105427669\n","step: 50700\n","learning_rate 0.000486778323107093\n","Step: 50700 -- Time: 203 => ppl: 10.923059772885875\n","step: 50800\n","learning_rate 0.0004846164646772349\n","Step: 50800 -- Time: 232 => ppl: 10.397745478623197\n","\n"," 50872 (array([20163,  3748]), array([ 82, 130]), array([2, 2])) , 43.142244182673416\n","\n","step: 50900\n","learning_rate 0.000482466326804727\n","Step: 50900 -- Time: 261 => ppl: 10.21915049329964\n","step: 51000\n","learning_rate 0.00048032783240890744\n","Step: 51000 -- Time: 289 => ppl: 10.248546437994017\n","\n","1.077132  1.9248606\n","\n","step: 51100\n","learning_rate 0.00047820090501730477\n","Step: 51100 -- Time: 321 => ppl: 10.011062347272818\n","step: 51200\n","learning_rate 0.0004760854687599593\n","Step: 51200 -- Time: 351 => ppl: 10.33230860686382\n","step: 51300\n","learning_rate 0.0004739814483638059\n","Step: 51300 -- Time: 383 => ppl: 10.014538646835158\n","step: 51400\n","learning_rate 0.00047188876914711786\n","Step: 51400 -- Time: 412 => ppl: 9.845883173150513\n","step: 51500\n","learning_rate 0.0004698073570140111\n","Step: 51500 -- Time: 442 => ppl: 10.255092786461981\n","step: 51600\n","learning_rate 0.00046773713844900803\n","Step: 51600 -- Time: 475 => ppl: 9.65523925344343\n","step: 51700\n","learning_rate 0.00046567804051166013\n","Step: 51700 -- Time: 505 => ppl: 10.156002816763726\n","step: 51800\n","learning_rate 0.0004636299908312285\n","Step: 51800 -- Time: 536 => ppl: 10.281703019715183\n","step: 51900\n","learning_rate 0.00046159291760142207\n","Step: 51900 -- Time: 566 => ppl: 10.85771476481941\n","step: 52000\n","learning_rate 0.0004595667495751922\n","Step: 52000 -- Time: 596 => ppl: 9.648308348542725\n","\n","1.6427404  3.2897766\n","\n","step: 52100\n","learning_rate 0.00045755141605958327\n","Step: 52100 -- Time: 627 => ppl: 10.117926332807366\n","step: 52200\n","learning_rate 0.0004555468469106386\n","Step: 52200 -- Time: 657 => ppl: 9.92181321281344\n","\n"," 52203 (array([20081, 20163]), array([539, 844]), array([2, 2])) , 58.105962201370666\n","\n","step: 52300\n","learning_rate 0.0004535529725283609\n","Step: 52300 -- Time: 687 => ppl: 10.163983754655428\n","step: 52400\n","learning_rate 0.00045156972385172674\n","Step: 52400 -- Time: 716 => ppl: 10.178784225030396\n","step: 52500\n","learning_rate 0.00044959703235375394\n","Step: 52500 -- Time: 744 => ppl: 9.947278167526942\n","step: 52600\n","learning_rate 0.00044763483003662193\n","Step: 52600 -- Time: 773 => ppl: 9.899197562026337\n","step: 52700\n","learning_rate 0.00044568304942684385\n","Step: 52700 -- Time: 803 => ppl: 9.978435206165074\n","step: 52800\n","learning_rate 0.00044374162357049\n","Step: 52800 -- Time: 831 => ppl: 10.569961887606295\n","step: 52900\n","learning_rate 0.00044181048602846183\n","Step: 52900 -- Time: 861 => ppl: 9.48642375847444\n","step: 53000\n","learning_rate 0.00043988957087181635\n","Step: 53000 -- Time: 889 => ppl: 9.774679410756294\n","\n","2.0760865  3.2121685\n","\n","step: 53100\n","learning_rate 0.0004379788126771396\n","Step: 53100 -- Time: 921 => ppl: 10.520074330494372\n","step: 53200\n","learning_rate 0.00043607814652196926\n","Step: 53200 -- Time: 952 => ppl: 10.430379350442053\n","step: 53300\n","learning_rate 0.00043418750798026527\n","Step: 53300 -- Time: 982 => ppl: 10.747466752588391\n","step: 53400\n","learning_rate 0.0004323068331179284\n","Step: 53400 -- Time: 1013 => ppl: 10.283918417568774\n","step: 53500\n","learning_rate 0.00043043605848836576\n","Step: 53500 -- Time: 1042 => ppl: 10.577189593964473\n","step: 53600\n","learning_rate 0.0004285751211281028\n","Step: 53600 -- Time: 1074 => ppl: 10.55804517365749\n","step: 53700\n","learning_rate 0.00042672395855244147\n","Step: 53700 -- Time: 1102 => ppl: 10.516483200828786\n","step: 53800\n","learning_rate 0.00042488250875116345\n","Step: 53800 -- Time: 1132 => ppl: 10.173445789627984\n","step: 53900\n","learning_rate 0.00042305071018427875\n","Step: 53900 -- Time: 1162 => ppl: 9.850894944027623\n","step: 54000\n","learning_rate 0.0004212285017778182\n","Step: 54000 -- Time: 1192 => ppl: 10.555676767449274\n","\n","1.1139672  2.5897307\n","\n","step: 54100\n","learning_rate 0.00041941582291967\n","Step: 54100 -- Time: 1223 => ppl: 9.987710695559535\n","step: 54200\n","learning_rate 0.00041761261345545943\n","Step: 54200 -- Time: 1254 => ppl: 10.643601623387772\n","\n"," 54285 (array([20163,  3795]), array([ 174, 1433]), array([2, 2])) , 68.92600576883886\n","\n","step: 54300\n","learning_rate 0.0004158188136844714\n","Step: 54300 -- Time: 1284 => ppl: 10.91626125624121\n","step: 54400\n","learning_rate 0.0004140343643556152\n","Step: 54400 -- Time: 1315 => ppl: 10.282055717430108\n","step: 54500\n","learning_rate 0.00041225920666343105\n","Step: 54500 -- Time: 1345 => ppl: 10.680279050604222\n","step: 54600\n","learning_rate 0.0004104932822441378\n","Step: 54600 -- Time: 1379 => ppl: 11.439127792534412\n","step: 54700\n","learning_rate 0.00040873653317172155\n","Step: 54700 -- Time: 1409 => ppl: 11.102016787294259\n","step: 54800\n","learning_rate 0.0004069889019540645\n","Step: 54800 -- Time: 1442 => ppl: 10.839812350508964\n","step: 54900\n","learning_rate 0.0004052503315291135\n","Step: 54900 -- Time: 1474 => ppl: 10.272021380124816\n","step: 55000\n","learning_rate 0.0004035207652610882\n","Step: 55000 -- Time: 1505 => ppl: 10.551216018684299\n","validation\n","### Step: 55000 -- Time: 1554 => ppl: 10.784892649094761\n","cnt 135\n","\n","1.1251531  2.589484\n","\n","step: 55100\n","learning_rate 0.00040180014693672766\n","Step: 55100 -- Time: 1590 => ppl: 10.372721171359514\n","step: 55200\n","learning_rate 0.00040008842076157575\n","Step: 55200 -- Time: 1623 => ppl: 10.590854242770131\n","step: 55300\n","learning_rate 0.0003983855313563045\n","Step: 55300 -- Time: 1653 => ppl: 9.903940343663036\n","step: 55400\n","learning_rate 0.00039669142375307474\n","Step: 55400 -- Time: 1686 => ppl: 10.681999106996255\n","step: 55500\n","learning_rate 0.0003950060433919339\n","Step: 55500 -- Time: 1717 => ppl: 10.501944650534265\n","step: 55600\n","learning_rate 0.00039332933611725066\n","Step: 55600 -- Time: 1747 => ppl: 10.203705893711833\n","step: 55700\n","learning_rate 0.00039166124817418554\n","Step: 55700 -- Time: 1779 => ppl: 11.098344993146503\n","step: 55800\n","learning_rate 0.00039000172620519744\n","Step: 55800 -- Time: 1810 => ppl: 11.056064232676336\n","step: 55900\n","learning_rate 0.00038835071724658567\n","Step: 55900 -- Time: 1840 => ppl: 10.229584777067538\n","step: 56000\n","learning_rate 0.0003867081687250666\n","Step: 56000 -- Time: 1869 => ppl: 10.259567151518029\n","\n","2.1452048  2.9404848\n","\n","step: 56100\n","learning_rate 0.0003850740284543853\n","Step: 56100 -- Time: 1898 => ppl: 10.551888422542685\n","step: 56200\n","learning_rate 0.000383448244631961\n","Step: 56200 -- Time: 1930 => ppl: 10.814368161035734\n","step: 56300\n","learning_rate 0.00038183076583556667\n","Step: 56300 -- Time: 1960 => ppl: 10.489692745515006\n","step: 56400\n","learning_rate 0.00038022154102004157\n","Step: 56400 -- Time: 1989 => ppl: 10.062827574794268\n","step: 56500\n","learning_rate 0.00037862051951403707\n","Step: 56500 -- Time: 2022 => ppl: 10.449577506426834\n","step: 56600\n","learning_rate 0.0003770276510167951\n","Step: 56600 -- Time: 2052 => ppl: 9.777343163498708\n","step: 56700\n","learning_rate 0.0003754428855949588\n","Step: 56700 -- Time: 2084 => ppl: 10.285609961778498\n","step: 56800\n","learning_rate 0.00037386617367941475\n","Step: 56800 -- Time: 2115 => ppl: 10.04461530577556\n","step: 56900\n","learning_rate 0.0003722974660621674\n","Step: 56900 -- Time: 2146 => ppl: 10.213126495272308\n","step: 57000\n","learning_rate 0.00037073671389324376\n","Step: 57000 -- Time: 2177 => ppl: 10.415668520432337\n","\n","1.3179679  2.3264148\n","\n","step: 57100\n","learning_rate 0.0003691838686776297\n","Step: 57100 -- Time: 2208 => ppl: 10.051666479534793\n","step: 57200\n","learning_rate 0.00036763888227223616\n","Step: 57200 -- Time: 2238 => ppl: 10.42119599036314\n","step: 57300\n","learning_rate 0.00036610170688289567\n","Step: 57300 -- Time: 2269 => ppl: 10.200136141173669\n","step: 57400\n","learning_rate 0.0003645722950613886\n","Step: 57400 -- Time: 2300 => ppl: 10.014664435467187\n","step: 57500\n","learning_rate 0.0003630505997024987\n","Step: 57500 -- Time: 2330 => ppl: 10.272482087379574\n","step: 57600\n","learning_rate 0.000361536574041098\n","Step: 57600 -- Time: 2361 => ppl: 9.954233507120115\n","step: 57700\n","learning_rate 0.0003600301716492601\n","Step: 57700 -- Time: 2392 => ppl: 10.206652832199227\n","step: 57800\n","learning_rate 0.0003585313464334022\n","Step: 57800 -- Time: 2424 => ppl: 9.923227463881991\n","step: 57900\n","learning_rate 0.0003570400526314551\n","Step: 57900 -- Time: 2453 => ppl: 10.202167048291988\n","step: 58000\n","learning_rate 0.00035555624481006104\n","Step: 58000 -- Time: 2484 => ppl: 9.845539929519191\n","\n","1.048343  1.9855947\n","\n","step: 58100\n","learning_rate 0.00035407987786179875\n","Step: 58100 -- Time: 2514 => ppl: 9.812427894666747\n","step: 58200\n","learning_rate 0.00035261090700243626\n","Step: 58200 -- Time: 2543 => ppl: 10.359893909349731\n","step: 58300\n","learning_rate 0.0003511492877682099\n","Step: 58300 -- Time: 2574 => ppl: 10.181575929761719\n","step: 58400\n","learning_rate 0.00034969497601313035\n","Step: 58400 -- Time: 2604 => ppl: 10.50085823102381\n","step: 58500\n","learning_rate 0.0003482479279063147\n","Step: 58500 -- Time: 2635 => ppl: 10.142797900551447\n","step: 58600\n","learning_rate 0.00034680809992934463\n","Step: 58600 -- Time: 2666 => ppl: 11.060494250017042\n","step: 58700\n","learning_rate 0.00034537544887365025\n","Step: 58700 -- Time: 2698 => ppl: 10.915416308566241\n","step: 58800\n","learning_rate 0.0003439499318379191\n","Step: 58800 -- Time: 2729 => ppl: 10.244053127504708\n","step: 58900\n","learning_rate 0.00034253150622553066\n","Step: 58900 -- Time: 2759 => ppl: 10.475979025492883\n","step: 59000\n","learning_rate 0.0003411201297420155\n","Step: 59000 -- Time: 2791 => ppl: 11.083945141912656\n","\n","1.1843005  2.7216537\n","\n","step: 59100\n","learning_rate 0.0003397157603925389\n","Step: 59100 -- Time: 2820 => ppl: 10.567651434357899\n","step: 59200\n","learning_rate 0.00033831835647940887\n","Step: 59200 -- Time: 2850 => ppl: 10.125184600376583\n","step: 59300\n","learning_rate 0.00033692787659960816\n","Step: 59300 -- Time: 2879 => ppl: 10.749593650456308\n","step: 59400\n","learning_rate 0.0003355442796423502\n","Step: 59400 -- Time: 2909 => ppl: 10.573246297580376\n","step: 59500\n","learning_rate 0.0003341675247866581\n","Step: 59500 -- Time: 2938 => ppl: 10.36763640734711\n","step: 59600\n","learning_rate 0.0003327975714989674\n","Step: 59600 -- Time: 2969 => ppl: 10.280765609804526\n","step: 59700\n","learning_rate 0.0003314343795307516\n","Step: 59700 -- Time: 2999 => ppl: 10.201956924074384\n","step: 59800\n","learning_rate 0.00033007790891617\n","Step: 59800 -- Time: 3030 => ppl: 11.201274024865182\n","step: 59900\n","learning_rate 0.0003287281199697389\n","Step: 59900 -- Time: 3060 => ppl: 10.554761264532834\n","step: 60000\n","learning_rate 0.0003273849732840243\n","Step: 60000 -- Time: 3089 => ppl: 10.167751644311238\n","validation\n","### Step: 60000 -- Time: 3140 => ppl: 10.925535479375633\n","cnt 136\n","\n","1.5233808  2.3999848\n","\n","step: 60100\n","learning_rate 0.0003260484297273567\n","Step: 60100 -- Time: 3176 => ppl: 10.445330983729075\n","step: 60200\n","learning_rate 0.00032471845044156804\n","Step: 60200 -- Time: 3204 => ppl: 10.283479850910664\n","step: 60300\n","learning_rate 0.0003233949968397498\n","Step: 60300 -- Time: 3235 => ppl: 10.155000844210964\n","step: 60400\n","learning_rate 0.00032207803060403244\n","Step: 60400 -- Time: 3266 => ppl: 10.369288462415426\n","step: 60500\n","learning_rate 0.00032076751368338593\n","Step: 60500 -- Time: 3297 => ppl: 10.67372647900241\n","step: 60600\n","learning_rate 0.0003194634082914411\n","Step: 60600 -- Time: 3325 => ppl: 10.233789420857397\n","step: 60700\n","learning_rate 0.0003181656769043317\n","Step: 60700 -- Time: 3354 => ppl: 9.891232358836092\n","step: 60800\n","learning_rate 0.0003168742822585566\n","Step: 60800 -- Time: 3383 => ppl: 10.470641033008247\n","step: 60900\n","learning_rate 0.0003155891873488623\n","Step: 60900 -- Time: 3413 => ppl: 9.515318232354913\n","step: 61000\n","learning_rate 0.00031431035542614527\n","Step: 61000 -- Time: 3443 => ppl: 10.845230599449684\n","\n","1.5398858  3.8029292\n","\n","step: 61100\n","learning_rate 0.0003130377499953742\n","Step: 61100 -- Time: 3472 => ppl: 10.324605706274935\n","step: 61200\n","learning_rate 0.0003117713348135316\n","Step: 61200 -- Time: 3501 => ppl: 10.129457468091283\n","step: 61300\n","learning_rate 0.00031051107388757474\n","Step: 61300 -- Time: 3529 => ppl: 9.860581427191182\n","step: 61400\n","learning_rate 0.00030925693147241563\n","Step: 61400 -- Time: 3558 => ppl: 9.507773986823029\n","step: 61500\n","learning_rate 0.0003080088720689201\n","Step: 61500 -- Time: 3585 => ppl: 9.430767154484641\n","step: 61600\n","learning_rate 0.00030676686042192515\n","Step: 61600 -- Time: 3613 => ppl: 10.64195088632765\n","step: 61700\n","learning_rate 0.0003055308615182754\n","Step: 61700 -- Time: 3641 => ppl: 10.119144273982808\n","step: 61800\n","learning_rate 0.00030430084058487706\n","Step: 61800 -- Time: 3670 => ppl: 10.297669311127823\n","step: 61900\n","learning_rate 0.00030307676308677085\n","Step: 61900 -- Time: 3697 => ppl: 9.92557496932269\n","step: 62000\n","learning_rate 0.0003018585947252221\n","Step: 62000 -- Time: 3725 => ppl: 9.702914280101405\n","\n","1.0784718  2.4644885\n","\n","step: 62100\n","learning_rate 0.0003006463014358289\n","Step: 62100 -- Time: 3752 => ppl: 9.88633259931666\n","step: 62200\n","learning_rate 0.00029943984938664774\n","Step: 62200 -- Time: 3780 => ppl: 10.20261514678205\n","step: 62300\n","learning_rate 0.0002982392049763367\n","Step: 62300 -- Time: 3807 => ppl: 9.847768114566422\n","step: 62400\n","learning_rate 0.00029704433483231514\n","Step: 62400 -- Time: 3834 => ppl: 9.929856399092483\n","step: 62500\n","learning_rate 0.00029585520580894116\n","Step: 62500 -- Time: 3863 => ppl: 10.1280936633266\n","step: 62600\n","learning_rate 0.0002946717849857054\n","Step: 62600 -- Time: 3890 => ppl: 9.598080683366806\n","step: 62700\n","learning_rate 0.0002934940396654415\n","Step: 62700 -- Time: 3918 => ppl: 10.615326686085185\n","step: 62800\n","learning_rate 0.00029232193737255307\n","Step: 62800 -- Time: 3945 => ppl: 9.604205843169094\n","step: 62900\n","learning_rate 0.0002911554458512571\n","Step: 62900 -- Time: 3971 => ppl: 8.971080327061927\n","step: 63000\n","learning_rate 0.000289994533063843\n","Step: 63000 -- Time: 3999 => ppl: 9.807552260774608\n","\n","1.1909666  1.947072\n","\n","step: 63100\n","learning_rate 0.00028883916718894806\n","Step: 63100 -- Time: 4028 => ppl: 9.53995589500094\n","step: 63200\n","learning_rate 0.0002876893166198486\n","Step: 63200 -- Time: 4054 => ppl: 9.293972444757667\n","step: 63300\n","learning_rate 0.0002865449499627663\n","Step: 63300 -- Time: 4081 => ppl: 8.913095020686201\n","step: 63400\n","learning_rate 0.00028540603603519055\n","Step: 63400 -- Time: 4107 => ppl: 8.84346728031768\n","step: 63500\n","learning_rate 0.0002842725438642157\n","Step: 63500 -- Time: 4135 => ppl: 9.097737907493347\n","step: 63600\n","learning_rate 0.0002831444426848936\n","Step: 63600 -- Time: 4160 => ppl: 8.96137636760817\n","step: 63700\n","learning_rate 0.0002820217019386009\n","Step: 63700 -- Time: 4187 => ppl: 9.8273440335688\n","step: 63800\n","learning_rate 0.00028090429127142154\n","Step: 63800 -- Time: 4213 => ppl: 9.770176683950123\n","step: 63900\n","learning_rate 0.00027979218053254335\n","Step: 63900 -- Time: 4238 => ppl: 9.127829493061032\n","step: 64000\n","learning_rate 0.0002786853397726697\n","Step: 64000 -- Time: 4263 => ppl: 9.828634335443969\n","\n","1.8868222  2.629467\n","\n","step: 64100\n","learning_rate 0.0002775837392424453\n","Step: 64100 -- Time: 4288 => ppl: 8.867219177941676\n","step: 64200\n","learning_rate 0.00027648734939089584\n","Step: 64200 -- Time: 4314 => ppl: 9.53358044185181\n","step: 64300\n","learning_rate 0.00027539614086388237\n","Step: 64300 -- Time: 4340 => ppl: 9.466821314771963\n","step: 64400\n","learning_rate 0.0002743100845025691\n","Step: 64400 -- Time: 4366 => ppl: 9.252544310494535\n","step: 64500\n","learning_rate 0.0002732291513419051\n","Step: 64500 -- Time: 4393 => ppl: 9.16019353345351\n","step: 64600\n","learning_rate 0.0002721533126091196\n","Step: 64600 -- Time: 4419 => ppl: 9.61865584646932\n","step: 64700\n","learning_rate 0.0002710825397222309\n","Step: 64700 -- Time: 4443 => ppl: 9.84823685110913\n","step: 64800\n","learning_rate 0.00027001680428856856\n","Step: 64800 -- Time: 4468 => ppl: 9.784282041070425\n","step: 64900\n","learning_rate 0.00026895607810330875\n","Step: 64900 -- Time: 4495 => ppl: 9.33387238977767\n","step: 65000\n","learning_rate 0.00026790033314802274\n","Step: 65000 -- Time: 4519 => ppl: 9.365214668592316\n","validation\n","### Step: 65000 -- Time: 4574 => ppl: 11.97744938281804\n","cnt 137\n","\n","1.7242248  2.908746\n","\n","step: 65100\n","learning_rate 0.0002668495415892383\n","Step: 65100 -- Time: 4602 => ppl: 9.57555761577211\n","step: 65200\n","learning_rate 0.00026580367577701405\n","Step: 65200 -- Time: 4627 => ppl: 9.283053499548062\n","step: 65300\n","learning_rate 0.00026476270824352626\n","Step: 65300 -- Time: 4651 => ppl: 9.662080042437534\n","step: 65400\n","learning_rate 0.0002637266117016684\n","Step: 65400 -- Time: 4674 => ppl: 9.318740418286731\n","step: 65500\n","learning_rate 0.0002626953590436632\n","Step: 65500 -- Time: 4700 => ppl: 9.557200599818936\n","step: 65600\n","learning_rate 0.0002616689233396865\n","Step: 65600 -- Time: 4723 => ppl: 10.108958370971365\n","step: 65700\n","learning_rate 0.000260647277836504\n","Step: 65700 -- Time: 4748 => ppl: 8.972092463956425\n","step: 65800\n","learning_rate 0.00025963039595611957\n","Step: 65800 -- Time: 4773 => ppl: 9.96932442409954\n","step: 65900\n","learning_rate 0.00025861825129443563\n","Step: 65900 -- Time: 4797 => ppl: 9.6169263976635\n","step: 66000\n","learning_rate 0.00025761081761992545\n","Step: 66000 -- Time: 4821 => ppl: 10.30752843062044\n","\n","2.0276656  3.401283\n","\n","step: 66100\n","learning_rate 0.00025660806887231694\n","Step: 66100 -- Time: 4844 => ppl: 9.89109353180473\n","step: 66200\n","learning_rate 0.0002556099791612882\n","Step: 66200 -- Time: 4868 => ppl: 9.834219405112579\n","step: 66300\n","learning_rate 0.0002546165227651744\n","Step: 66300 -- Time: 4893 => ppl: 10.348419700496931\n","step: 66400\n","learning_rate 0.0002536276741296862\n","Step: 66400 -- Time: 4916 => ppl: 9.677317285376304\n","step: 66500\n","learning_rate 0.00025264340786663905\n","Step: 66500 -- Time: 4940 => ppl: 9.330745879680165\n","step: 66600\n","learning_rate 0.00025166369875269406\n","Step: 66600 -- Time: 4963 => ppl: 9.753802213511817\n","step: 66700\n","learning_rate 0.0002506885217281095\n","Step: 66700 -- Time: 4987 => ppl: 9.538151939013567\n","step: 66800\n","learning_rate 0.00024971785189550346\n","Step: 66800 -- Time: 5011 => ppl: 9.354643085177276\n","step: 66900\n","learning_rate 0.00024875166451862715\n","Step: 66900 -- Time: 5036 => ppl: 9.910840051073304\n","step: 67000\n","learning_rate 0.00024778993502114894\n","Step: 67000 -- Time: 5059 => ppl: 9.233123846924213\n","\n","1.3255959  2.7666306\n","\n","step: 67100\n","learning_rate 0.0002468326389854489\n","Step: 67100 -- Time: 5083 => ppl: 8.946868900965868\n","step: 67200\n","learning_rate 0.0002458797521514241\n","Step: 67200 -- Time: 5107 => ppl: 9.571621822691208\n","step: 67300\n","learning_rate 0.00024493125041530377\n","Step: 67300 -- Time: 5132 => ppl: 9.653983417172476\n","step: 67400\n","learning_rate 0.00024398710982847523\n","Step: 67400 -- Time: 5155 => ppl: 9.281278336214465\n","step: 67500\n","learning_rate 0.00024304730659631974\n","Step: 67500 -- Time: 5179 => ppl: 9.589210650541144\n","step: 67600\n","learning_rate 0.00024211181707705842\n","Step: 67600 -- Time: 5204 => ppl: 9.384074407644952\n","step: 67700\n","learning_rate 0.0002411806177806082\n","Step: 67700 -- Time: 5229 => ppl: 9.478774332166697\n","step: 67800\n","learning_rate 0.00024025368536744761\n","Step: 67800 -- Time: 5253 => ppl: 10.015252049263765\n","step: 67900\n","learning_rate 0.00023933099664749232\n","Step: 67900 -- Time: 5278 => ppl: 9.711427755551442\n","step: 68000\n","learning_rate 0.00023841252857898038\n","Step: 68000 -- Time: 5303 => ppl: 9.605731628186057\n","\n","1.577893  2.2847319\n","\n","step: 68100\n","learning_rate 0.00023749825826736712\n","Step: 68100 -- Time: 5328 => ppl: 9.497364475704488\n","step: 68200\n","learning_rate 0.00023658816296422936\n","Step: 68200 -- Time: 5352 => ppl: 9.583724810975822\n","step: 68300\n","learning_rate 0.0002356822200661791\n","Step: 68300 -- Time: 5377 => ppl: 9.051400385782685\n","step: 68400\n","learning_rate 0.0002347804071137866\n","Step: 68400 -- Time: 5400 => ppl: 9.45454610955407\n","step: 68500\n","learning_rate 0.00023388270179051262\n","Step: 68500 -- Time: 5424 => ppl: 9.619329355403027\n","step: 68600\n","learning_rate 0.00023298908192164963\n","Step: 68600 -- Time: 5450 => ppl: 9.42315048387175\n","step: 68700\n","learning_rate 0.00023209952547327225\n","Step: 68700 -- Time: 5474 => ppl: 9.664252876964433\n","step: 68800\n","learning_rate 0.00023121401055119662\n","Step: 68800 -- Time: 5499 => ppl: 9.339933050215178\n","step: 68900\n","learning_rate 0.0002303325153999485\n","Step: 68900 -- Time: 5522 => ppl: 9.290453249103196\n","step: 69000\n","learning_rate 0.00022945501840174022\n","Step: 69000 -- Time: 5547 => ppl: 9.118047767805809\n","\n","1.9112989  3.0359375\n","\n","step: 69100\n","learning_rate 0.00022858149807545637\n","Step: 69100 -- Time: 5571 => ppl: 9.562107277113695\n","step: 69200\n","learning_rate 0.00022771193307564799\n","Step: 69200 -- Time: 5595 => ppl: 9.493458837320214\n","step: 69300\n","learning_rate 0.00022684630219153536\n","Step: 69300 -- Time: 5620 => ppl: 9.670846319234442\n","step: 69400\n","learning_rate 0.0002259845843460192\n","Step: 69400 -- Time: 5645 => ppl: 9.126526633269126\n","step: 69500\n","learning_rate 0.0002251267585947002\n","Step: 69500 -- Time: 5669 => ppl: 8.696332470969157\n","step: 69600\n","learning_rate 0.0002242728041249069\n","Step: 69600 -- Time: 5694 => ppl: 9.261384845602143\n","step: 69700\n","learning_rate 0.00022342270025473167\n","Step: 69700 -- Time: 5719 => ppl: 9.392322540304198\n","step: 69800\n","learning_rate 0.0002225764264320749\n","Step: 69800 -- Time: 5743 => ppl: 9.023907842271909\n","step: 69900\n","learning_rate 0.0002217339622336971\n","Step: 69900 -- Time: 5767 => ppl: 9.529724968073326\n","step: 70000\n","learning_rate 0.00022089528736427924\n","Step: 70000 -- Time: 5791 => ppl: 8.82302171534317\n","validation\n","### Step: 70000 -- Time: 5845 => ppl: 11.828249471558088\n","cnt 138\n","\n","0.9543478  2.033316\n","\n","step: 70100\n","learning_rate 0.00022006038165549064\n","Step: 70100 -- Time: 5875 => ppl: 9.74404003640812\n","step: 70200\n","learning_rate 0.00021922922506506482\n","Step: 70200 -- Time: 5899 => ppl: 9.44634264203209\n","step: 70300\n","learning_rate 0.0002184017976758832\n","Step: 70300 -- Time: 5924 => ppl: 8.957603051078513\n","step: 70400\n","learning_rate 0.00021757807969506626\n","Step: 70400 -- Time: 5948 => ppl: 9.095059753777484\n","step: 70500\n","learning_rate 0.0002167580514530724\n","Step: 70500 -- Time: 5972 => ppl: 9.053583598950336\n","step: 70600\n","learning_rate 0.00021594169340280442\n","Step: 70600 -- Time: 5996 => ppl: 9.070576730791664\n","step: 70700\n","learning_rate 0.00021512898611872317\n","Step: 70700 -- Time: 6020 => ppl: 8.82549444209377\n","step: 70800\n","learning_rate 0.00021431991029596882\n","Step: 70800 -- Time: 6044 => ppl: 8.526200212565328\n","step: 70900\n","learning_rate 0.0002135144467494894\n","Step: 70900 -- Time: 6068 => ppl: 8.903432963597481\n","step: 71000\n","learning_rate 0.0002127125764131764\n","Step: 71000 -- Time: 6092 => ppl: 8.93178072691658\n","\n","2.1518579  2.994166\n","\n","step: 71100\n","learning_rate 0.0002119142803390079\n","Step: 71100 -- Time: 6116 => ppl: 8.40168190379025\n","step: 71200\n","learning_rate 0.00021111953969619836\n","Step: 71200 -- Time: 6139 => ppl: 8.4554056199327\n","step: 71300\n","learning_rate 0.00021032833577035577\n","Step: 71300 -- Time: 6162 => ppl: 8.96763892859785\n","step: 71400\n","learning_rate 0.0002095406499626456\n","Step: 71400 -- Time: 6186 => ppl: 8.436554191855077\n","step: 71500\n","learning_rate 0.00020875646378896187\n","Step: 71500 -- Time: 6211 => ppl: 8.83081230473892\n","step: 71600\n","learning_rate 0.00020797575887910468\n","Step: 71600 -- Time: 6235 => ppl: 8.635530623736967\n","step: 71700\n","learning_rate 0.0002071985169759649\n","Step: 71700 -- Time: 6259 => ppl: 9.211929212221014\n","step: 71800\n","learning_rate 0.00020642471993471533\n","Step: 71800 -- Time: 6283 => ppl: 8.704961401575146\n","step: 71900\n","learning_rate 0.00020565434972200867\n","Step: 71900 -- Time: 6307 => ppl: 8.92847478304147\n","step: 72000\n","learning_rate 0.00020488738841518189\n","Step: 72000 -- Time: 6331 => ppl: 8.795780907863692\n","\n","1.1389015  2.6705835\n","\n","step: 72100\n","learning_rate 0.0002041238182014673\n","Step: 72100 -- Time: 6356 => ppl: 8.964051124700768\n","step: 72200\n","learning_rate 0.00020336362137721\n","Step: 72200 -- Time: 6381 => ppl: 8.663183048885935\n","step: 72300\n","learning_rate 0.0002026067803470917\n","Step: 72300 -- Time: 6406 => ppl: 8.707621710967247\n","step: 72400\n","learning_rate 0.00020185327762336095\n","Step: 72400 -- Time: 6431 => ppl: 8.766203955956946\n","step: 72500\n","learning_rate 0.00020110309582506972\n","Step: 72500 -- Time: 6456 => ppl: 8.698782924654243\n","step: 72600\n","learning_rate 0.00020035621767731596\n","Step: 72600 -- Time: 6480 => ppl: 8.731475406862371\n","step: 72700\n","learning_rate 0.0001996126260104926\n","Step: 72700 -- Time: 6505 => ppl: 9.245387159144059\n","step: 72800\n","learning_rate 0.00019887230375954253\n","Step: 72800 -- Time: 6530 => ppl: 9.289510786961351\n","\n"," 72819 (array([20326,  3811]), array([1639, 1038]), array([2, 2])) , 74.36415974938744\n","\n","step: 72900\n","learning_rate 0.00019813523396321965\n","Step: 72900 -- Time: 6553 => ppl: 8.786969124438942\n","step: 73000\n","learning_rate 0.00019740139976335587\n","Step: 73000 -- Time: 6578 => ppl: 8.91420814782334\n","\n","1.0515947  1.941626\n","\n","step: 73100\n","learning_rate 0.00019667078440413423\n","Step: 73100 -- Time: 6603 => ppl: 9.21484311172391\n","step: 73200\n","learning_rate 0.00019594337123136766\n","Step: 73200 -- Time: 6628 => ppl: 8.746459433472015\n","step: 73300\n","learning_rate 0.0001952191436917838\n","Step: 73300 -- Time: 6654 => ppl: 9.469760942124068\n","step: 73400\n","learning_rate 0.00019449808533231539\n","Step: 73400 -- Time: 6678 => ppl: 8.951339273794499\n","step: 73500\n","learning_rate 0.00019378017979939665\n","Step: 73500 -- Time: 6705 => ppl: 9.618953116743493\n","step: 73600\n","learning_rate 0.0001930654108382651\n","Step: 73600 -- Time: 6729 => ppl: 8.782508406331477\n","step: 73700\n","learning_rate 0.00019235376229226903\n","Step: 73700 -- Time: 6753 => ppl: 8.803786743816154\n","step: 73800\n","learning_rate 0.00019164521810218074\n","Step: 73800 -- Time: 6778 => ppl: 8.971566550989662\n","step: 73900\n","learning_rate 0.00019093976230551504\n","Step: 73900 -- Time: 6803 => ppl: 8.99708810570322\n","step: 74000\n","learning_rate 0.00019023737903585346\n","Step: 74000 -- Time: 6827 => ppl: 8.56802073099772\n","\n","1.0700133  2.7053475\n","\n","step: 74100\n","learning_rate 0.00018953805252217365\n","Step: 74100 -- Time: 6852 => ppl: 8.45452162927241\n","step: 74200\n","learning_rate 0.00018884176708818434\n","Step: 74200 -- Time: 6877 => ppl: 8.652789198527413\n","step: 74300\n","learning_rate 0.00018814850715166554\n","Step: 74300 -- Time: 6901 => ppl: 9.333386265626721\n","step: 74400\n","learning_rate 0.000187458257223814\n","Step: 74400 -- Time: 6924 => ppl: 9.421968206880017\n","step: 74500\n","learning_rate 0.0001867710019085939\n","Step: 74500 -- Time: 6948 => ppl: 8.452452082518645\n","step: 74600\n","learning_rate 0.00018608672590209285\n","Step: 74600 -- Time: 6973 => ppl: 9.404109965471246\n","step: 74700\n","learning_rate 0.00018540541399188277\n","Step: 74700 -- Time: 6998 => ppl: 8.954765094040201\n","step: 74800\n","learning_rate 0.00018472705105638607\n","Step: 74800 -- Time: 7021 => ppl: 8.588527484794632\n","step: 74900\n","learning_rate 0.00018405162206424678\n","Step: 74900 -- Time: 7046 => ppl: 9.147416053609978\n","step: 75000\n","learning_rate 0.00018337911207370669\n","Step: 75000 -- Time: 7070 => ppl: 9.115277161026857\n","validation\n","### Step: 75000 -- Time: 7121 => ppl: 11.5327047934571\n","cnt 139\n","\n","1.5257763  3.5351918\n","\n","step: 75100\n","learning_rate 0.00018270950623198643\n","Step: 75100 -- Time: 7150 => ppl: 8.753002143837163\n","step: 75200\n","learning_rate 0.0001820427897746715\n","Step: 75200 -- Time: 7174 => ppl: 8.971862786074574\n","step: 75300\n","learning_rate 0.00018137894802510313\n","Step: 75300 -- Time: 7197 => ppl: 8.836163723089532\n","step: 75400\n","learning_rate 0.0001807179663937738\n","Step: 75400 -- Time: 7221 => ppl: 8.538316272104543\n","step: 75500\n","learning_rate 0.00018005983037772794\n","Step: 75500 -- Time: 7245 => ppl: 9.018115156292447\n","step: 75600\n","learning_rate 0.00017940452555996686\n","Step: 75600 -- Time: 7271 => ppl: 8.527212458359033\n","step: 75700\n","learning_rate 0.00017875203760885878\n","Step: 75700 -- Time: 7294 => ppl: 8.710291563665614\n","step: 75800\n","learning_rate 0.00017810235227755323\n","Step: 75800 -- Time: 7318 => ppl: 8.766590012848711\n","step: 75900\n","learning_rate 0.00017745545540340027\n","Step: 75900 -- Time: 7343 => ppl: 8.721037085925929\n","step: 76000\n","learning_rate 0.00017681133290737403\n","Step: 76000 -- Time: 7368 => ppl: 8.819114168902702\n","\n","1.1021674  4.1632037\n","\n","step: 76100\n","learning_rate 0.00017616997079350106\n","Step: 76100 -- Time: 7393 => ppl: 8.965043652768964\n","step: 76200\n","learning_rate 0.0001755313551482929\n","Step: 76200 -- Time: 7417 => ppl: 9.243629227254939\n","step: 76300\n","learning_rate 0.00017489547214018326\n","Step: 76300 -- Time: 7442 => ppl: 9.027435029134384\n","step: 76400\n","learning_rate 0.00017426230801896952\n","Step: 76400 -- Time: 7467 => ppl: 9.502879744715294\n","step: 76500\n","learning_rate 0.00017363184911525865\n","Step: 76500 -- Time: 7492 => ppl: 8.923417657948049\n","step: 76600\n","learning_rate 0.00017300408183991742\n","Step: 76600 -- Time: 7518 => ppl: 9.309824986797143\n","step: 76700\n","learning_rate 0.00017237899268352693\n","Step: 76700 -- Time: 7543 => ppl: 9.598204532382528\n","step: 76800\n","learning_rate 0.00017175656821584128\n","Step: 76800 -- Time: 7568 => ppl: 9.151885607869023\n","step: 76900\n","learning_rate 0.00017113679508525063\n","Step: 76900 -- Time: 7594 => ppl: 9.911382982835557\n","step: 77000\n","learning_rate 0.00017051966001824833\n","Step: 77000 -- Time: 7620 => ppl: 9.20627230789296\n","\n","1.3166732  2.0856056\n","\n","step: 77100\n","learning_rate 0.00016990514981890214\n","Step: 77100 -- Time: 7647 => ppl: 9.385956001030134\n","step: 77200\n","learning_rate 0.00016929325136832965\n","Step: 77200 -- Time: 7673 => ppl: 9.119683748643801\n","step: 77300\n","learning_rate 0.00016868395162417774\n","Step: 77300 -- Time: 7699 => ppl: 9.064840733497144\n","step: 77400\n","learning_rate 0.00016807723762010606\n","Step: 77400 -- Time: 7726 => ppl: 9.018487492170374\n","step: 77500\n","learning_rate 0.00016747309646527442\n","Step: 77500 -- Time: 7751 => ppl: 9.028986957172652\n","step: 77600\n","learning_rate 0.00016687151534383433\n","Step: 77600 -- Time: 7777 => ppl: 9.36084542084036\n","step: 77700\n","learning_rate 0.00016627248151442428\n","Step: 77700 -- Time: 7803 => ppl: 9.369482050250593\n","step: 77800\n","learning_rate 0.000165675982309669\n","Step: 77800 -- Time: 7829 => ppl: 9.03344416250373\n","step: 77900\n","learning_rate 0.00016508200513568259\n","Step: 77900 -- Time: 7855 => ppl: 9.391849963788603\n","step: 78000\n","learning_rate 0.00016449053747157538\n","Step: 78000 -- Time: 7881 => ppl: 8.615632323186903\n","\n","1.2532358  2.4506555\n","\n","step: 78100\n","learning_rate 0.0001639015668689647\n","Step: 78100 -- Time: 7906 => ppl: 9.230087537223305\n","step: 78200\n","learning_rate 0.00016331508095148933\n","Step: 78200 -- Time: 7932 => ppl: 9.37143472281093\n","step: 78300\n","learning_rate 0.00016273106741432762\n","Step: 78300 -- Time: 7958 => ppl: 8.976144912683615\n","step: 78400\n","learning_rate 0.0001621495140237195\n","Step: 78400 -- Time: 7984 => ppl: 9.334372669367701\n","step: 78500\n","learning_rate 0.00016157040861649192\n","Step: 78500 -- Time: 8010 => ppl: 8.850756060524494\n","step: 78600\n","learning_rate 0.00016099373909958807\n","Step: 78600 -- Time: 8035 => ppl: 8.985885090151218\n","step: 78700\n","learning_rate 0.0001604194934496001\n","Step: 78700 -- Time: 8060 => ppl: 8.657926519745523\n","step: 78800\n","learning_rate 0.00015984765971230547\n","Step: 78800 -- Time: 8086 => ppl: 9.772987722795882\n","step: 78900\n","learning_rate 0.00015927822600220683\n","Step: 78900 -- Time: 8111 => ppl: 8.652850522267356\n","step: 79000\n","learning_rate 0.00015871118050207533\n","Step: 79000 -- Time: 8134 => ppl: 8.550663909911266\n","\n","1.9923121  3.274806\n","\n","step: 79100\n","learning_rate 0.0001581465114624975\n","Step: 79100 -- Time: 8161 => ppl: 9.377608425398295\n","step: 79200\n","learning_rate 0.00015758420720142553\n","Step: 79200 -- Time: 8186 => ppl: 8.80439701820475\n","step: 79300\n","learning_rate 0.00015702425610373096\n","Step: 79300 -- Time: 8212 => ppl: 8.726869716926052\n","step: 79400\n","learning_rate 0.00015646664662076173\n","Step: 79400 -- Time: 8238 => ppl: 9.101456100364627\n","step: 79500\n","learning_rate 0.00015591136726990262\n","Step: 79500 -- Time: 8262 => ppl: 8.906542166291471\n","step: 79600\n","learning_rate 0.00015535840663413907\n","Step: 79600 -- Time: 8288 => ppl: 9.42095173856465\n","step: 79700\n","learning_rate 0.00015480775336162414\n","Step: 79700 -- Time: 8314 => ppl: 9.430123686612887\n","step: 79800\n","learning_rate 0.00015425939616524892\n","Step: 79800 -- Time: 8338 => ppl: 9.113548870795439\n","step: 79900\n","learning_rate 0.00015371332382221607\n","Step: 79900 -- Time: 8364 => ppl: 9.371044346229478\n","step: 80000\n","learning_rate 0.00015316952517361652\n","Step: 80000 -- Time: 8391 => ppl: 9.576738393077758\n","validation\n","### Step: 80000 -- Time: 8441 => ppl: 11.418648039306806\n","cnt 140\n","\n","0.8437691  1.7959403\n","\n","step: 80100\n","learning_rate 0.00015262798912400957\n","Step: 80100 -- Time: 8472 => ppl: 9.30680621891778\n","step: 80200\n","learning_rate 0.00015208870464100596\n","Step: 80200 -- Time: 8497 => ppl: 9.010241878499848\n","step: 80300\n","learning_rate 0.00015155166075485408\n","Step: 80300 -- Time: 8522 => ppl: 9.328987627892701\n","step: 80400\n","learning_rate 0.00015101684655802943\n","Step: 80400 -- Time: 8549 => ppl: 9.022982831627594\n","step: 80500\n","learning_rate 0.00015048425120482695\n","Step: 80500 -- Time: 8574 => ppl: 8.618850369434126\n","step: 80600\n","learning_rate 0.0001499538639109566\n","Step: 80600 -- Time: 8601 => ppl: 8.9363293040176\n","step: 80700\n","learning_rate 0.0001494256739531419\n","Step: 80700 -- Time: 8628 => ppl: 9.13005135928757\n","step: 80800\n","learning_rate 0.00014889967066872135\n","Step: 80800 -- Time: 8652 => ppl: 8.788483553769883\n","step: 80900\n","learning_rate 0.00014837584345525303\n","Step: 80900 -- Time: 8678 => ppl: 9.123247708174047\n","step: 81000\n","learning_rate 0.00014785418177012204\n","Step: 81000 -- Time: 8703 => ppl: 9.299841343085545\n","\n","4.296922  5.936321\n","\n","step: 81100\n","learning_rate 0.00014733467513015078\n","Step: 81100 -- Time: 8727 => ppl: 9.657964209405238\n","step: 81200\n","learning_rate 0.00014681731311121237\n","Step: 81200 -- Time: 8752 => ppl: 9.50714491184187\n","step: 81300\n","learning_rate 0.0001463020853478467\n","Step: 81300 -- Time: 8776 => ppl: 8.821650689579418\n","step: 81400\n","learning_rate 0.00014578898153287946\n","Step: 81400 -- Time: 8802 => ppl: 9.49412645891952\n","step: 81500\n","learning_rate 0.00014527799141704403\n","Step: 81500 -- Time: 8826 => ppl: 9.061067673828342\n","step: 81600\n","learning_rate 0.00014476910480860607\n","Step: 81600 -- Time: 8851 => ppl: 9.16001605656409\n","step: 81700\n","learning_rate 0.00014426231157299094\n","Step: 81700 -- Time: 8877 => ppl: 8.631983380636697\n","step: 81800\n","learning_rate 0.0001437576016324139\n","Step: 81800 -- Time: 8901 => ppl: 9.314226949565318\n","step: 81900\n","learning_rate 0.00014325496496551298\n","Step: 81900 -- Time: 8927 => ppl: 9.77901757807569\n","step: 82000\n","learning_rate 0.00014275439160698472\n","Step: 82000 -- Time: 8951 => ppl: 9.692632321817047\n","\n","2.6705754  3.9735887\n","\n","step: 82100\n","learning_rate 0.00014225587164722235\n","Step: 82100 -- Time: 8977 => ppl: 9.745576247168964\n","step: 82200\n","learning_rate 0.00014175939523195685\n","Step: 82200 -- Time: 9003 => ppl: 8.99853886910436\n","step: 82300\n","learning_rate 0.00014126495256190055\n","Step: 82300 -- Time: 9029 => ppl: 9.589607651120462\n","step: 82400\n","learning_rate 0.00014077253389239343\n","Step: 82400 -- Time: 9054 => ppl: 9.316758801356512\n","step: 82500\n","learning_rate 0.00014028212953305186\n","Step: 82500 -- Time: 9078 => ppl: 9.448796384909075\n","step: 82600\n","learning_rate 0.0001397937298474201\n","Step: 82600 -- Time: 9102 => ppl: 9.955649171101776\n","step: 82700\n","learning_rate 0.00013930732525262437\n","Step: 82700 -- Time: 9125 => ppl: 8.743735275175272\n","step: 82800\n","learning_rate 0.00013882290621902916\n","Step: 82800 -- Time: 9150 => ppl: 9.196392287541476\n","step: 82900\n","learning_rate 0.00013834046326989646\n","Step: 82900 -- Time: 9175 => ppl: 9.778582654212704\n","step: 83000\n","learning_rate 0.00013785998698104726\n","Step: 83000 -- Time: 9199 => ppl: 9.109731404671443\n","\n","1.4041712  2.4054232\n","\n","step: 83100\n","learning_rate 0.00013738146798052543\n","Step: 83100 -- Time: 9223 => ppl: 9.660216139539099\n","step: 83200\n","learning_rate 0.0001369048969482643\n","Step: 83200 -- Time: 9248 => ppl: 9.48113316542994\n","step: 83300\n","learning_rate 0.00013643026461575557\n","Step: 83300 -- Time: 9272 => ppl: 8.852717226322172\n","step: 83400\n","learning_rate 0.0001359575617657205\n","Step: 83400 -- Time: 9297 => ppl: 8.849834034977565\n","step: 83500\n","learning_rate 0.0001354867792317837\n","Step: 83500 -- Time: 9321 => ppl: 8.940799648564411\n","step: 83600\n","learning_rate 0.00013501790789814918\n","Step: 83600 -- Time: 9344 => ppl: 8.842343821985233\n","step: 83700\n","learning_rate 0.00013455093869927875\n","Step: 83700 -- Time: 9368 => ppl: 9.032736440492842\n","step: 83800\n","learning_rate 0.0001340858626195728\n","Step: 83800 -- Time: 9391 => ppl: 9.42665731940094\n","step: 83900\n","learning_rate 0.00013362267069305336\n","Step: 83900 -- Time: 9416 => ppl: 9.841432177274829\n","step: 84000\n","learning_rate 0.00013316135400304952\n","Step: 84000 -- Time: 9439 => ppl: 9.07728759368431\n","\n","1.4317808  4.2419906\n","\n","step: 84100\n","learning_rate 0.00013270190368188504\n","Step: 84100 -- Time: 9462 => ppl: 9.382891668230839\n","step: 84200\n","learning_rate 0.00013224431091056818\n","Step: 84200 -- Time: 9485 => ppl: 9.59201847729712\n","step: 84300\n","learning_rate 0.00013178856691848403\n","Step: 84300 -- Time: 9510 => ppl: 9.559733756435717\n","step: 84400\n","learning_rate 0.00013133466298308872\n","Step: 84400 -- Time: 9536 => ppl: 10.448405653822162\n","step: 84500\n","learning_rate 0.00013088259042960606\n","Step: 84500 -- Time: 9564 => ppl: 10.184176011368196\n","step: 84600\n","learning_rate 0.00013043234063072628\n","Step: 84600 -- Time: 9588 => ppl: 9.461580384652136\n","step: 84700\n","learning_rate 0.00012998390500630695\n","Step: 84700 -- Time: 9617 => ppl: 9.633611129460194\n","step: 84800\n","learning_rate 0.00012953727502307613\n","Step: 84800 -- Time: 9645 => ppl: 10.013540379204182\n","step: 84900\n","learning_rate 0.0001290924421943375\n","Step: 84900 -- Time: 9674 => ppl: 10.161519026900534\n","step: 85000\n","learning_rate 0.00012864939807967764\n","Step: 85000 -- Time: 9702 => ppl: 9.94537134394476\n","validation\n","### Step: 85000 -- Time: 9753 => ppl: 10.810197059396799\n","cnt 141\n","\n","1.7156557  3.3544624\n","\n","step: 85100\n","learning_rate 0.00012820813428467563\n","Step: 85100 -- Time: 9788 => ppl: 10.105958722753252\n","step: 85200\n","learning_rate 0.0001277686424606144\n","Step: 85200 -- Time: 9818 => ppl: 10.190455710021823\n","step: 85300\n","learning_rate 0.00012733091430419438\n","Step: 85300 -- Time: 9845 => ppl: 9.758454217130604\n","step: 85400\n","learning_rate 0.00012689494155724906\n","Step: 85400 -- Time: 9879 => ppl: 10.795156100205904\n","step: 85500\n","learning_rate 0.0001264607160064627\n","Step: 85500 -- Time: 9908 => ppl: 10.47257048612562\n","step: 85600\n","learning_rate 0.00012602822948308998\n","Step: 85600 -- Time: 9937 => ppl: 9.947471382709097\n","step: 85700\n","learning_rate 0.0001255974738626776\n","Step: 85700 -- Time: 9968 => ppl: 10.520156207293494\n","step: 85800\n","learning_rate 0.00012516844106478804\n","Step: 85800 -- Time: 9998 => ppl: 10.225885884319673\n","step: 85900\n","learning_rate 0.00012474112305272506\n","Step: 85900 -- Time: 10028 => ppl: 10.472485007781142\n","step: 86000\n","learning_rate 0.00012431551183326138\n","Step: 86000 -- Time: 10063 => ppl: 10.291781392070604\n","\n","1.6709541  2.6993768\n","\n","step: 86100\n","learning_rate 0.00012389159945636809\n","Step: 86100 -- Time: 10098 => ppl: 10.683054789866459\n","step: 86200\n","learning_rate 0.00012346937801494608\n","Step: 86200 -- Time: 10131 => ppl: 10.912361111332238\n","step: 86300\n","learning_rate 0.0001230488396445594\n","Step: 86300 -- Time: 10162 => ppl: 10.087476171777663\n","step: 86400\n","learning_rate 0.00012262997652317043\n","Step: 86400 -- Time: 10197 => ppl: 10.763469127773046\n","step: 86500\n","learning_rate 0.00012221278087087704\n","Step: 86500 -- Time: 10234 => ppl: 11.210255874775742\n","step: 86600\n","learning_rate 0.00012179724494965142\n","Step: 86600 -- Time: 10270 => ppl: 10.50433437406586\n","step: 86700\n","learning_rate 0.00012138336106308099\n","Step: 86700 -- Time: 10306 => ppl: 10.845260136161516\n","step: 86800\n","learning_rate 0.00012097112155611093\n","Step: 86800 -- Time: 10342 => ppl: 10.327561483131872\n","step: 86900\n","learning_rate 0.00012056051881478864\n","Step: 86900 -- Time: 10378 => ppl: 10.466213573352208\n","\n"," 86993 (array([ 4085, 20432]), array([101, 557]), array([2, 2])) , 88.90717241442793\n","\n","step: 87000\n","learning_rate 0.00012015154526601\n","Step: 87000 -- Time: 10413 => ppl: 10.646267235827139\n","\n","2.1184545  2.9198396\n","\n","step: 87100\n","learning_rate 0.00011974419337726738\n","Step: 87100 -- Time: 10448 => ppl: 10.917715559255907\n","step: 87200\n","learning_rate 0.00011933845565639937\n","Step: 87200 -- Time: 10481 => ppl: 10.18087466358182\n","step: 87300\n","learning_rate 0.00011893432465134244\n","Step: 87300 -- Time: 10512 => ppl: 9.715549730692182\n","step: 87400\n","learning_rate 0.00011853179294988412\n","Step: 87400 -- Time: 10545 => ppl: 9.831272888837297\n","step: 87500\n","learning_rate 0.00011813085317941809\n","Step: 87500 -- Time: 10579 => ppl: 9.945749173561644\n","step: 87600\n","learning_rate 0.0001177314980067009\n","Step: 87600 -- Time: 10611 => ppl: 9.59632592820229\n","step: 87700\n","learning_rate 0.00011733372013761044\n","Step: 87700 -- Time: 10643 => ppl: 10.754153302833691\n","step: 87800\n","learning_rate 0.00011693751231690602\n","Step: 87800 -- Time: 10676 => ppl: 9.601367966568235\n","step: 87900\n","learning_rate 0.00011654286732799017\n","Step: 87900 -- Time: 10709 => ppl: 10.911174460728626\n","step: 88000\n","learning_rate 0.0001161497779926722\n","Step: 88000 -- Time: 10743 => ppl: 10.372758712128194\n","\n","2.5129845  3.8950312\n","\n","step: 88100\n","learning_rate 0.00011575823717093323\n","Step: 88100 -- Time: 10777 => ppl: 10.225149037553473\n","step: 88200\n","learning_rate 0.00011536823776069295\n","Step: 88200 -- Time: 10807 => ppl: 9.900462595425491\n","step: 88300\n","learning_rate 0.00011497977269757805\n","Step: 88300 -- Time: 10842 => ppl: 10.377657387759502\n","step: 88400\n","learning_rate 0.00011459283495469218\n","Step: 88400 -- Time: 10874 => ppl: 10.140458627710306\n","step: 88500\n","learning_rate 0.00011420741754238753\n","Step: 88500 -- Time: 10903 => ppl: 9.530576115452874\n","step: 88600\n","learning_rate 0.00011382351350803801\n","Step: 88600 -- Time: 10939 => ppl: 10.26010403320759\n","step: 88700\n","learning_rate 0.00011344111593581398\n","Step: 88700 -- Time: 10974 => ppl: 10.018449433580859\n","step: 88800\n","learning_rate 0.00011306021794645862\n","Step: 88800 -- Time: 11004 => ppl: 10.102893815023448\n","step: 88900\n","learning_rate 0.0001126808126970657\n","Step: 88900 -- Time: 11034 => ppl: 9.70248569926671\n","step: 89000\n","learning_rate 0.00011230289338085905\n","Step: 89000 -- Time: 11067 => ppl: 9.998888948172148\n","\n","1.423662  2.4884126\n","\n","step: 89100\n","learning_rate 0.0001119264532269734\n","Step: 89100 -- Time: 11100 => ppl: 9.638208142287933\n","step: 89200\n","learning_rate 0.00011155148550023691\n","Step: 89200 -- Time: 11137 => ppl: 10.134406308710503\n","step: 89300\n","learning_rate 0.00011117798350095499\n","Step: 89300 -- Time: 11168 => ppl: 9.795088621171136\n","step: 89400\n","learning_rate 0.0001108059405646958\n","Step: 89400 -- Time: 11201 => ppl: 9.998800412540938\n","step: 89500\n","learning_rate 0.00011043535006207711\n","Step: 89500 -- Time: 11234 => ppl: 9.811077994599462\n","step: 89600\n","learning_rate 0.00011006620539855469\n","Step: 89600 -- Time: 11269 => ppl: 9.844912398407155\n","step: 89700\n","learning_rate 0.00010969850001421212\n","Step: 89700 -- Time: 11300 => ppl: 10.019220147366118\n","step: 89800\n","learning_rate 0.00010933222738355203\n","Step: 89800 -- Time: 11331 => ppl: 9.242627393448853\n","step: 89900\n","learning_rate 0.00010896738101528883\n","Step: 89900 -- Time: 11366 => ppl: 10.018363721433808\n","step: 90000\n","learning_rate 0.00010860395445214283\n","Step: 90000 -- Time: 11398 => ppl: 9.593717231369233\n","validation\n","### Step: 90000 -- Time: 11451 => ppl: 10.93290075259584\n","cnt 142\n","\n","0.9360182  2.00718\n","\n","step: 90100\n","learning_rate 0.00010824194127063568\n","Step: 90100 -- Time: 11488 => ppl: 9.983662896850843\n","step: 90200\n","learning_rate 0.00010788133508088743\n","Step: 90200 -- Time: 11522 => ppl: 10.141738653868549\n","step: 90300\n","learning_rate 0.00010752212952641476\n","Step: 90300 -- Time: 11557 => ppl: 9.731826274225778\n","step: 90400\n","learning_rate 0.00010716431828393067\n","Step: 90400 -- Time: 11590 => ppl: 10.138553082766018\n","step: 90500\n","learning_rate 0.00010680789506314559\n","Step: 90500 -- Time: 11622 => ppl: 10.090218404801726\n","step: 90600\n","learning_rate 0.00010645285360656972\n","Step: 90600 -- Time: 11656 => ppl: 9.814026486485805\n","step: 90700\n","learning_rate 0.00010609918768931689\n","Step: 90700 -- Time: 11690 => ppl: 9.284541958382242\n","step: 90800\n","learning_rate 0.00010574689111890957\n","Step: 90800 -- Time: 11725 => ppl: 9.802599330486904\n","step: 90900\n","learning_rate 0.00010539595773508535\n","Step: 90900 -- Time: 11762 => ppl: 10.595219565010693\n","step: 91000\n","learning_rate 0.00010504638140960465\n","Step: 91000 -- Time: 11796 => ppl: 10.22902957049739\n","\n","1.282629  2.059409\n","\n","step: 91100\n","learning_rate 0.00010469815604605977\n","Step: 91100 -- Time: 11832 => ppl: 9.888731907375611\n","step: 91200\n","learning_rate 0.00010435127557968527\n","Step: 91200 -- Time: 11866 => ppl: 9.846377375180726\n","step: 91300\n","learning_rate 0.0001040057339771695\n","Step: 91300 -- Time: 11901 => ppl: 10.022510251336923\n","step: 91400\n","learning_rate 0.00010366152523646762\n","Step: 91400 -- Time: 11936 => ppl: 9.694503881140019\n","step: 91500\n","learning_rate 0.00010331864338661569\n","Step: 91500 -- Time: 11969 => ppl: 9.944793122147697\n","step: 91600\n","learning_rate 0.00010297708248754609\n","Step: 91600 -- Time: 12004 => ppl: 10.06842774401839\n","step: 91700\n","learning_rate 0.00010263683662990424\n","Step: 91700 -- Time: 12035 => ppl: 9.30205554954576\n","step: 91800\n","learning_rate 0.00010229789993486648\n","Step: 91800 -- Time: 12069 => ppl: 10.065903766103412\n","step: 91900\n","learning_rate 0.00010196026655395921\n","Step: 91900 -- Time: 12100 => ppl: 9.757169250521464\n","step: 92000\n","learning_rate 0.00010162393066887921\n","Step: 92000 -- Time: 12131 => ppl: 9.7717864547983\n","\n","1.9169436  3.1357098\n","\n","step: 92100\n","learning_rate 0.00010128888649131529\n","Step: 92100 -- Time: 12162 => ppl: 10.186140934630943\n","step: 92200\n","learning_rate 0.00010095512826277093\n","Step: 92200 -- Time: 12195 => ppl: 10.333040701542267\n","step: 92300\n","learning_rate 0.00010062265025438834\n","Step: 92300 -- Time: 12227 => ppl: 9.972137009433801\n","step: 92400\n","learning_rate 0.00010029144676677355\n","Step: 92400 -- Time: 12261 => ppl: 10.226183906296265\n","step: 92500\n","learning_rate 9.996151212982272e-05\n","Step: 92500 -- Time: 12293 => ppl: 9.952627759995611\n","step: 92600\n","learning_rate 9.963284070254962e-05\n","Step: 92600 -- Time: 12323 => ppl: 9.414669047236641\n","step: 92700\n","learning_rate 9.930542687291425e-05\n","Step: 92700 -- Time: 12355 => ppl: 10.223259069999445\n","step: 92800\n","learning_rate 9.897926505765264e-05\n","Step: 92800 -- Time: 12389 => ppl: 9.760782688155402\n","step: 92900\n","learning_rate 9.86543497021077e-05\n","Step: 92900 -- Time: 12420 => ppl: 9.796535247764501\n","step: 93000\n","learning_rate 9.833067528006136e-05\n","Step: 93000 -- Time: 12450 => ppl: 10.138449387292876\n","\n","1.4400884  1.860705\n","\n","step: 93100\n","learning_rate 9.800823629356763e-05\n","Step: 93100 -- Time: 12482 => ppl: 10.05512993417208\n","step: 93200\n","learning_rate 9.768702727278694e-05\n","Step: 93200 -- Time: 12513 => ppl: 9.439584849058116\n","step: 93300\n","learning_rate 9.73670427758215e-05\n","Step: 93300 -- Time: 12543 => ppl: 9.463893422786617\n","step: 93400\n","learning_rate 9.70482773885518e-05\n","Step: 93400 -- Time: 12575 => ppl: 10.838598974559806\n","step: 93500\n","learning_rate 9.673072572447409e-05\n","Step: 93500 -- Time: 12608 => ppl: 9.808949803670357\n","step: 93600\n","learning_rate 9.641438242453915e-05\n","Step: 93600 -- Time: 12644 => ppl: 10.026220098719373\n","step: 93700\n","learning_rate 9.60992421569919e-05\n","Step: 93700 -- Time: 12675 => ppl: 9.986476069913987\n","step: 93800\n","learning_rate 9.57852996172122e-05\n","Step: 93800 -- Time: 12707 => ppl: 10.386050506068623\n","step: 93900\n","learning_rate 9.54725495275567e-05\n","Step: 93900 -- Time: 12738 => ppl: 9.7378081069703\n","step: 94000\n","learning_rate 9.516098663720172e-05\n","Step: 94000 -- Time: 12771 => ppl: 9.660438797488894\n","\n","1.8529232  3.7466803\n","\n","step: 94100\n","learning_rate 9.485060572198718e-05\n","Step: 94100 -- Time: 12802 => ppl: 10.709164671970766\n","step: 94200\n","learning_rate 9.454140158426148e-05\n","Step: 94200 -- Time: 12836 => ppl: 10.399414481976052\n","step: 94300\n","learning_rate 9.423336905272756e-05\n","Step: 94300 -- Time: 12869 => ppl: 9.92679442107586\n","step: 94400\n","learning_rate 9.392650298228987e-05\n","Step: 94400 -- Time: 12899 => ppl: 10.040593402414094\n","step: 94500\n","learning_rate 9.362079825390234e-05\n","Step: 94500 -- Time: 12932 => ppl: 9.971506303538144\n","step: 94600\n","learning_rate 9.331624977441739e-05\n","Step: 94600 -- Time: 12964 => ppl: 10.02953179428514\n","step: 94700\n","learning_rate 9.301285247643597e-05\n","Step: 94700 -- Time: 12998 => ppl: 10.560407671466189\n","step: 94800\n","learning_rate 9.271060131815849e-05\n","Step: 94800 -- Time: 13034 => ppl: 10.072134771788559\n","step: 94900\n","learning_rate 9.24094912832368e-05\n","Step: 94900 -- Time: 13070 => ppl: 10.515238896964274\n","step: 95000\n","learning_rate 9.210951738062717e-05\n","Step: 95000 -- Time: 13106 => ppl: 10.271679941460443\n","validation\n","### Step: 95000 -- Time: 13156 => ppl: 10.436410123627418\n","cnt 143\n","\n","1.520732  3.3339515\n","\n","step: 95100\n","learning_rate 9.181067464444409e-05\n","Step: 95100 -- Time: 13196 => ppl: 10.01159118826172\n","step: 95200\n","learning_rate 9.151295813381522e-05\n","Step: 95200 -- Time: 13225 => ppl: 9.94491544623935\n","step: 95300\n","learning_rate 9.121636293273716e-05\n","Step: 95300 -- Time: 13256 => ppl: 10.116506995239913\n","step: 95400\n","learning_rate 9.09208841499322e-05\n","Step: 95400 -- Time: 13287 => ppl: 10.426873936173749\n","step: 95500\n","learning_rate 9.062651691870602e-05\n","Step: 95500 -- Time: 13316 => ppl: 10.100090318269752\n","step: 95600\n","learning_rate 9.033325639680632e-05\n","Step: 95600 -- Time: 13346 => ppl: 10.124650403236155\n","step: 95700\n","learning_rate 9.004109776628233e-05\n","Step: 95700 -- Time: 13377 => ppl: 10.034012950330586\n","step: 95800\n","learning_rate 8.975003623334529e-05\n","Step: 95800 -- Time: 13404 => ppl: 10.145035626772941\n","step: 95900\n","learning_rate 8.94600670282298e-05\n","Step: 95900 -- Time: 13431 => ppl: 10.09715542503853\n","step: 96000\n","learning_rate 8.917118540505611e-05\n","Step: 96000 -- Time: 13461 => ppl: 10.222033048821265\n","\n","1.1871306  2.510455\n","\n","step: 96100\n","learning_rate 8.888338664169332e-05\n","Step: 96100 -- Time: 13489 => ppl: 10.08212705859536\n","step: 96200\n","learning_rate 8.859666603962335e-05\n","Step: 96200 -- Time: 13517 => ppl: 10.100241259049719\n","step: 96300\n","learning_rate 8.831101892380592e-05\n","Step: 96300 -- Time: 13544 => ppl: 9.716401241123398\n","step: 96400\n","learning_rate 8.802644064254444e-05\n","Step: 96400 -- Time: 13572 => ppl: 9.756331120891767\n","step: 96500\n","learning_rate 8.774292656735256e-05\n","Step: 96500 -- Time: 13600 => ppl: 9.390762958879282\n","step: 96600\n","learning_rate 8.746047209282185e-05\n","Step: 96600 -- Time: 13628 => ppl: 9.268910649381231\n","step: 96700\n","learning_rate 8.71790726364901e-05\n","Step: 96700 -- Time: 13657 => ppl: 10.10088864582013\n","step: 96800\n","learning_rate 8.689872363871067e-05\n","Step: 96800 -- Time: 13686 => ppl: 10.50646448121371\n","step: 96900\n","learning_rate 8.661942056252254e-05\n","Step: 96900 -- Time: 13714 => ppl: 9.878480791138719\n","step: 97000\n","learning_rate 8.634115889352128e-05\n","Step: 97000 -- Time: 13743 => ppl: 8.99902624664506\n","\n","1.7181312  2.48978\n","\n","step: 97100\n","learning_rate 8.606393413973081e-05\n","Step: 97100 -- Time: 13772 => ppl: 9.461296108464648\n","step: 97200\n","learning_rate 8.578774183147602e-05\n","Step: 97200 -- Time: 13799 => ppl: 9.504490773721834\n","step: 97300\n","learning_rate 8.551257752125618e-05\n","Step: 97300 -- Time: 13826 => ppl: 9.415702288557227\n","step: 97400\n","learning_rate 8.523843678361917e-05\n","Step: 97400 -- Time: 13855 => ppl: 9.604167322313799\n","step: 97500\n","learning_rate 8.49653152150366e-05\n","Step: 97500 -- Time: 13882 => ppl: 8.933081996904384\n","step: 97600\n","learning_rate 8.469320843377958e-05\n","Step: 97600 -- Time: 13911 => ppl: 9.58411361364527\n","step: 97700\n","learning_rate 8.44221120797954e-05\n","Step: 97700 -- Time: 13942 => ppl: 10.036366702641212\n","step: 97800\n","learning_rate 8.415202181458495e-05\n","Step: 97800 -- Time: 13968 => ppl: 8.87918955988752\n","step: 97900\n","learning_rate 8.3882933321081e-05\n","Step: 97900 -- Time: 13996 => ppl: 9.792635527322167\n","step: 98000\n","learning_rate 8.361484230352718e-05\n","Step: 98000 -- Time: 14024 => ppl: 9.226172601991745\n","\n","1.416464  3.2801845\n","\n","step: 98100\n","learning_rate 8.334774448735774e-05\n","Step: 98100 -- Time: 14052 => ppl: 9.697369975034842\n","step: 98200\n","learning_rate 8.308163561907811e-05\n","Step: 98200 -- Time: 14080 => ppl: 9.664782747498878\n","step: 98300\n","learning_rate 8.281651146614625e-05\n","Step: 98300 -- Time: 14110 => ppl: 9.605659804949097\n","step: 98400\n","learning_rate 8.25523678168547e-05\n","Step: 98400 -- Time: 14139 => ppl: 9.694200863617827\n","step: 98500\n","learning_rate 8.228920048021336e-05\n","Step: 98500 -- Time: 14168 => ppl: 10.045173750039329\n","step: 98600\n","learning_rate 8.202700528583316e-05\n","Step: 98600 -- Time: 14199 => ppl: 10.535305330583299\n","step: 98700\n","learning_rate 8.17657780838103e-05\n","Step: 98700 -- Time: 14227 => ppl: 10.494604127651117\n","step: 98800\n","learning_rate 8.15055147446113e-05\n","Step: 98800 -- Time: 14255 => ppl: 10.295800317526416\n","step: 98900\n","learning_rate 8.124621115895884e-05\n","Step: 98900 -- Time: 14282 => ppl: 9.581890714378485\n","step: 99000\n","learning_rate 8.09878632377182e-05\n","Step: 99000 -- Time: 14311 => ppl: 10.21747770867687\n","\n","7.782111  9.634964\n","\n","step: 99100\n","learning_rate 8.073046691178451e-05\n","Step: 99100 -- Time: 14343 => ppl: 10.535365649633173\n","step: 99200\n","learning_rate 8.047401813197077e-05\n","Step: 99200 -- Time: 14374 => ppl: 10.684119175773247\n","step: 99300\n","learning_rate 8.021851286889637e-05\n","Step: 99300 -- Time: 14403 => ppl: 10.582231752822985\n","step: 99400\n","learning_rate 7.996394711287656e-05\n","Step: 99400 -- Time: 14434 => ppl: 10.169757675189219\n","step: 99500\n","learning_rate 7.971031687381246e-05\n","Step: 99500 -- Time: 14464 => ppl: 9.985173808843331\n","step: 99600\n","learning_rate 7.945761818108183e-05\n","Step: 99600 -- Time: 14494 => ppl: 9.194475023948003\n","step: 99700\n","learning_rate 7.92058470834305e-05\n","Step: 99700 -- Time: 14523 => ppl: 9.920844690282115\n","step: 99800\n","learning_rate 7.895499964886454e-05\n","Step: 99800 -- Time: 14552 => ppl: 9.642788178204928\n","step: 99900\n","learning_rate 7.870507196454305e-05\n","Step: 99900 -- Time: 14583 => ppl: 10.025704228463805\n","step: 100000\n","learning_rate 7.845606013667164e-05\n","Step: 100000 -- Time: 14614 => ppl: 10.392879734633576\n","validation\n","### Step: 100000 -- Time: 14661 => ppl: 10.69466456611991\n","cnt 144\n","\n","2.2146864  3.8929915\n","\n","step: 100100\n","learning_rate 7.820796029039662e-05\n","Step: 100100 -- Time: 14695 => ppl: 10.009402721264513\n","step: 100200\n","learning_rate 7.796076856969978e-05\n","Step: 100200 -- Time: 14726 => ppl: 10.153000635300064\n","step: 100300\n","learning_rate 7.771448113729394e-05\n","Step: 100300 -- Time: 14756 => ppl: 9.97818313894728\n","step: 100400\n","learning_rate 7.746909417451904e-05\n","Step: 100400 -- Time: 14787 => ppl: 10.112715057669062\n","step: 100500\n","learning_rate 7.722460388123898e-05\n","Step: 100500 -- Time: 14819 => ppl: 10.56303767253748\n","step: 100600\n","learning_rate 7.698100647573901e-05\n","Step: 100600 -- Time: 14848 => ppl: 9.67817741651791\n","step: 100700\n","learning_rate 7.673829819462391e-05\n","Step: 100700 -- Time: 14876 => ppl: 9.867910898421906\n","step: 100800\n","learning_rate 7.649647529271662e-05\n","Step: 100800 -- Time: 14905 => ppl: 10.080323772398813\n","step: 100900\n","learning_rate 7.625553404295766e-05\n","Step: 100900 -- Time: 14935 => ppl: 10.220563459927368\n","step: 101000\n","learning_rate 7.601547073630512e-05\n","Step: 101000 -- Time: 14966 => ppl: 9.902619748465046\n","\n","0.88616186  1.824677\n","\n","step: 101100\n","learning_rate 7.577628168163521e-05\n","Step: 101100 -- Time: 14996 => ppl: 9.625308879743711\n","step: 101200\n","learning_rate 7.553796320564361e-05\n","Step: 101200 -- Time: 15028 => ppl: 10.284810326035293\n","step: 101300\n","learning_rate 7.530051165274724e-05\n","Step: 101300 -- Time: 15058 => ppl: 10.158004389327925\n","step: 101400\n","learning_rate 7.506392338498676e-05\n","Step: 101400 -- Time: 15088 => ppl: 9.369716781795658\n","step: 101500\n","learning_rate 7.48281947819296e-05\n","Step: 101500 -- Time: 15118 => ppl: 9.715365114410869\n","step: 101600\n","learning_rate 7.45933222405737e-05\n","Step: 101600 -- Time: 15148 => ppl: 10.020825774856176\n","step: 101700\n","learning_rate 7.435930217525175e-05\n","Step: 101700 -- Time: 15180 => ppl: 9.66903888852853\n","step: 101800\n","learning_rate 7.412613101753608e-05\n","Step: 101800 -- Time: 15211 => ppl: 10.026643054257248\n","step: 101900\n","learning_rate 7.38938052161441e-05\n","Step: 101900 -- Time: 15241 => ppl: 10.53556441157293\n","step: 102000\n","learning_rate 7.366232123684438e-05\n","Step: 102000 -- Time: 15274 => ppl: 10.522047696140042\n","\n","2.2658014  3.4727201\n","\n","step: 102100\n","learning_rate 7.343167556236323e-05\n","Step: 102100 -- Time: 15306 => ppl: 10.728362711395258\n","step: 102200\n","learning_rate 7.320186469229199e-05\n","Step: 102200 -- Time: 15336 => ppl: 10.394502667408492\n","step: 102300\n","learning_rate 7.297288514299475e-05\n","Step: 102300 -- Time: 15367 => ppl: 10.45114536245771\n","step: 102400\n","learning_rate 7.274473344751674e-05\n","Step: 102400 -- Time: 15396 => ppl: 10.160078529907867\n","step: 102500\n","learning_rate 7.251740615549325e-05\n","Step: 102500 -- Time: 15426 => ppl: 10.383459904111836\n","step: 102600\n","learning_rate 7.229089983305911e-05\n","Step: 102600 -- Time: 15453 => ppl: 10.753025572173762\n","step: 102700\n","learning_rate 7.206521106275872e-05\n","Step: 102700 -- Time: 15484 => ppl: 10.51614957277286\n","step: 102800\n","learning_rate 7.18403364434567e-05\n","Step: 102800 -- Time: 15517 => ppl: 10.517192306393213\n","step: 102900\n","learning_rate 7.161627259024898e-05\n","Step: 102900 -- Time: 15548 => ppl: 11.616862569598217\n","step: 103000\n","learning_rate 7.139301613437456e-05\n","Step: 103000 -- Time: 15579 => ppl: 11.005435979057994\n","\n","0.8465564  2.1742446\n","\n","step: 103100\n","learning_rate 7.117056372312769e-05\n","Step: 103100 -- Time: 15610 => ppl: 10.191044510260085\n","step: 103200\n","learning_rate 7.094891201977073e-05\n","Step: 103200 -- Time: 15640 => ppl: 10.066665595224473\n","step: 103300\n","learning_rate 7.072805770344737e-05\n","Step: 103300 -- Time: 15672 => ppl: 10.301461397261464\n","step: 103400\n","learning_rate 7.05079974690966e-05\n","Step: 103400 -- Time: 15702 => ppl: 9.854297761324432\n","step: 103500\n","learning_rate 7.028872802736696e-05\n","Step: 103500 -- Time: 15731 => ppl: 10.203627516837454\n","step: 103600\n","learning_rate 7.007024610453158e-05\n","Step: 103600 -- Time: 15760 => ppl: 10.396834085818247\n","step: 103700\n","learning_rate 6.985254844240348e-05\n","Step: 103700 -- Time: 15790 => ppl: 10.46875435629759\n","step: 103800\n","learning_rate 6.963563179825161e-05\n","Step: 103800 -- Time: 15820 => ppl: 10.66465356218802\n","step: 103900\n","learning_rate 6.941949294471725e-05\n","Step: 103900 -- Time: 15850 => ppl: 10.11666135791331\n","step: 104000\n","learning_rate 6.920412866973102e-05\n","Step: 104000 -- Time: 15881 => ppl: 10.155319674975942\n","\n","1.3561991  2.3983371\n","\n","step: 104100\n","learning_rate 6.898953577643033e-05\n","Step: 104100 -- Time: 15910 => ppl: 10.440693905053179\n","step: 104200\n","learning_rate 6.877571108307737e-05\n","Step: 104200 -- Time: 15940 => ppl: 10.741347976737785\n","step: 104300\n","learning_rate 6.856265142297758e-05\n","Step: 104300 -- Time: 15969 => ppl: 9.882415040696937\n","step: 104400\n","learning_rate 6.835035364439869e-05\n","Step: 104400 -- Time: 15999 => ppl: 10.104347336113921\n","step: 104500\n","learning_rate 6.813881461049006e-05\n","Step: 104500 -- Time: 16030 => ppl: 10.329943525548206\n","step: 104600\n","learning_rate 6.792803119920277e-05\n","Step: 104600 -- Time: 16061 => ppl: 10.504760928618174\n","step: 104700\n","learning_rate 6.771800030321004e-05\n","Step: 104700 -- Time: 16090 => ppl: 10.143568759485719\n","step: 104800\n","learning_rate 6.750871882982812e-05\n","Step: 104800 -- Time: 16118 => ppl: 9.884874395175846\n","step: 104900\n","learning_rate 6.73001837009377e-05\n","Step: 104900 -- Time: 16149 => ppl: 10.270861750275845\n","step: 105000\n","learning_rate 6.709239185290588e-05\n","Step: 105000 -- Time: 16177 => ppl: 9.66851751668561\n","validation\n","### Step: 105000 -- Time: 16223 => ppl: 10.307337563195844\n","cnt 145\n","\n","2.8027115  5.813927\n","\n","step: 105100\n","learning_rate 6.688534023650842e-05\n","Step: 105100 -- Time: 16257 => ppl: 9.770016362441503\n","step: 105200\n","learning_rate 6.667902581685269e-05\n","Step: 105200 -- Time: 16285 => ppl: 9.615192024063322\n","step: 105300\n","learning_rate 6.647344557330087e-05\n","Step: 105300 -- Time: 16312 => ppl: 9.621372552632\n","step: 105400\n","learning_rate 6.626859649939378e-05\n","Step: 105400 -- Time: 16341 => ppl: 9.48594426821916\n","step: 105500\n","learning_rate 6.606447560277507e-05\n","Step: 105500 -- Time: 16369 => ppl: 9.421721165930876\n","step: 105600\n","learning_rate 6.586107990511592e-05\n","Step: 105600 -- Time: 16398 => ppl: 9.745380600028843\n","step: 105700\n","learning_rate 6.565840644204017e-05\n","Step: 105700 -- Time: 16427 => ppl: 9.624309497437018\n","step: 105800\n","learning_rate 6.545645226304993e-05\n","Step: 105800 -- Time: 16456 => ppl: 9.793439389330278\n","step: 105900\n","learning_rate 6.52552144314516e-05\n","Step: 105900 -- Time: 16484 => ppl: 10.679241219845057\n","step: 106000\n","learning_rate 6.505469002428233e-05\n","Step: 106000 -- Time: 16514 => ppl: 9.278207959864964\n","\n","1.3356167  2.58611\n","\n","step: 106100\n","learning_rate 6.485487613223702e-05\n","Step: 106100 -- Time: 16542 => ppl: 10.092963251398036\n","step: 106200\n","learning_rate 6.465576985959558e-05\n","Step: 106200 -- Time: 16570 => ppl: 10.326131424077222\n","step: 106300\n","learning_rate 6.445736832415085e-05\n","Step: 106300 -- Time: 16598 => ppl: 10.054446449360245\n","step: 106400\n","learning_rate 6.425966865713674e-05\n","Step: 106400 -- Time: 16628 => ppl: 10.306826239337381\n","step: 106500\n","learning_rate 6.406266800315694e-05\n","Step: 106500 -- Time: 16656 => ppl: 10.714267211503484\n","step: 106600\n","learning_rate 6.386636352011402e-05\n","Step: 106600 -- Time: 16685 => ppl: 9.919545364918815\n","step: 106700\n","learning_rate 6.367075237913894e-05\n","Step: 106700 -- Time: 16713 => ppl: 10.86182462775408\n","step: 106800\n","learning_rate 6.347583176452096e-05\n","Step: 106800 -- Time: 16744 => ppl: 10.618044764639368\n","step: 106900\n","learning_rate 6.328159887363806e-05\n","Step: 106900 -- Time: 16771 => ppl: 10.093430114372877\n","step: 107000\n","learning_rate 6.30880509168877e-05\n","Step: 107000 -- Time: 16798 => ppl: 10.492622184296188\n","\n","0.96845424  1.7591531\n","\n","step: 107100\n","learning_rate 6.289518511761794e-05\n","Step: 107100 -- Time: 16826 => ppl: 10.253945641756756\n","step: 107200\n","learning_rate 6.270299871205918e-05\n","Step: 107200 -- Time: 16854 => ppl: 10.48960755212506\n","step: 107300\n","learning_rate 6.251148894925601e-05\n","Step: 107300 -- Time: 16883 => ppl: 10.498516723775648\n","step: 107400\n","learning_rate 6.232065309099978e-05\n","Step: 107400 -- Time: 16910 => ppl: 10.548799443213467\n","step: 107500\n","learning_rate 6.213048841176129e-05\n","Step: 107500 -- Time: 16937 => ppl: 9.929354996062342\n","step: 107600\n","learning_rate 6.194099219862409e-05\n","Step: 107600 -- Time: 16966 => ppl: 10.333884911300546\n","step: 107700\n","learning_rate 6.175216175121805e-05\n","Step: 107700 -- Time: 16994 => ppl: 9.832279780039944\n","step: 107800\n","learning_rate 6.15639943816534e-05\n","Step: 107800 -- Time: 17021 => ppl: 10.092845210935009\n","step: 107900\n","learning_rate 6.13764874144551e-05\n","Step: 107900 -- Time: 17049 => ppl: 10.36324061710317\n","step: 108000\n","learning_rate 6.118963818649765e-05\n","Step: 108000 -- Time: 17076 => ppl: 10.3181449342038\n","\n","1.4492061  2.2809036\n","\n","step: 108100\n","learning_rate 6.1003444046940296e-05\n","Step: 108100 -- Time: 17103 => ppl: 10.548213453969288\n","step: 108200\n","learning_rate 6.0817902357162526e-05\n","Step: 108200 -- Time: 17132 => ppl: 9.951802748194662\n","step: 108300\n","learning_rate 6.063301049070008e-05\n","Step: 108300 -- Time: 17159 => ppl: 10.692495587163108\n","step: 108400\n","learning_rate 6.044876583318122e-05\n","Step: 108400 -- Time: 17189 => ppl: 10.518733081232261\n","step: 108500\n","learning_rate 6.026516578226347e-05\n","Step: 108500 -- Time: 17217 => ppl: 10.328216664130643\n","step: 108600\n","learning_rate 6.008220774757067e-05\n","Step: 108600 -- Time: 17245 => ppl: 10.842503621441894\n","step: 108700\n","learning_rate 5.9899889150630426e-05\n","Step: 108700 -- Time: 17272 => ppl: 9.91453872771899\n","step: 108800\n","learning_rate 5.971820742481192e-05\n","Step: 108800 -- Time: 17300 => ppl: 11.004608642972107\n","step: 108900\n","learning_rate 5.953716001526409e-05\n","Step: 108900 -- Time: 17327 => ppl: 9.734086337784637\n","step: 109000\n","learning_rate 5.9356744378854196e-05\n","Step: 109000 -- Time: 17356 => ppl: 10.639864390783366\n","\n","1.0210979  1.7698774\n","\n","step: 109100\n","learning_rate 5.9176957984106705e-05\n","Step: 109100 -- Time: 17383 => ppl: 10.673465041151069\n","step: 109200\n","learning_rate 5.899779831114257e-05\n","Step: 109200 -- Time: 17411 => ppl: 10.501384571157448\n","step: 109300\n","learning_rate 5.881926285161885e-05\n","Step: 109300 -- Time: 17438 => ppl: 10.137751434944581\n","step: 109400\n","learning_rate 5.864134910866868e-05\n","Step: 109400 -- Time: 17466 => ppl: 10.661622099402193\n","step: 109500\n","learning_rate 5.846405459684163e-05\n","Step: 109500 -- Time: 17495 => ppl: 10.510160719384857\n","step: 109600\n","learning_rate 5.828737684204436e-05\n","Step: 109600 -- Time: 17524 => ppl: 11.26095499906642\n","step: 109700\n","learning_rate 5.811131338148165e-05\n","Step: 109700 -- Time: 17554 => ppl: 10.747273832448885\n","step: 109800\n","learning_rate 5.793586176359781e-05\n","Step: 109800 -- Time: 17580 => ppl: 10.663803513548045\n","step: 109900\n","learning_rate 5.776101954801836e-05\n","Step: 109900 -- Time: 17608 => ppl: 10.930742712447403\n","step: 110000\n","learning_rate 5.75867843054921e-05\n","Step: 110000 -- Time: 17636 => ppl: 11.099338763940086\n","validation\n","### Step: 110000 -- Time: 17683 => ppl: 10.689606830951451\n","cnt 146\n","\n","1.4018257  2.262141\n","\n","step: 110100\n","learning_rate 5.7413153617833515e-05\n","Step: 110100 -- Time: 17717 => ppl: 10.27445428511138\n","step: 110200\n","learning_rate 5.7240125077865505e-05\n","Step: 110200 -- Time: 17745 => ppl: 10.830929233036198\n","step: 110300\n","learning_rate 5.7067696289362465e-05\n","Step: 110300 -- Time: 17773 => ppl: 11.15256840531703\n","step: 110400\n","learning_rate 5.689586486699368e-05\n","Step: 110400 -- Time: 17802 => ppl: 10.560175230281802\n","step: 110500\n","learning_rate 5.6724628436267036e-05\n","Step: 110500 -- Time: 17832 => ppl: 11.515238909956345\n","step: 110600\n","learning_rate 5.655398463347312e-05\n","Step: 110600 -- Time: 17859 => ppl: 11.033254144390401\n","step: 110700\n","learning_rate 5.6383931105629575e-05\n","Step: 110700 -- Time: 17888 => ppl: 10.680412274692406\n","step: 110800\n","learning_rate 5.621446551042582e-05\n","Step: 110800 -- Time: 17918 => ppl: 10.327230554612303\n","step: 110900\n","learning_rate 5.604558551616808e-05\n","Step: 110900 -- Time: 17947 => ppl: 10.788580269478555\n","step: 111000\n","learning_rate 5.587728880172473e-05\n","Step: 111000 -- Time: 17975 => ppl: 10.753471937349717\n","\n","1.858048  3.044199\n","\n","step: 111100\n","learning_rate 5.570957305647197e-05\n","Step: 111100 -- Time: 18005 => ppl: 10.782338960313963\n","step: 111200\n","learning_rate 5.554243598023981e-05\n","Step: 111200 -- Time: 18033 => ppl: 10.324472757243063\n","step: 111300\n","learning_rate 5.537587528325835e-05\n","Step: 111300 -- Time: 18061 => ppl: 10.495386191987194\n","step: 111400\n","learning_rate 5.5209888686104414e-05\n","Step: 111400 -- Time: 18091 => ppl: 11.260535524272408\n","step: 111500\n","learning_rate 5.504447391964843e-05\n","Step: 111500 -- Time: 18120 => ppl: 10.494712367714929\n","step: 111600\n","learning_rate 5.487962872500168e-05\n","Step: 111600 -- Time: 18150 => ppl: 10.16070878780549\n","step: 111700\n","learning_rate 5.471535085346383e-05\n","Step: 111700 -- Time: 18180 => ppl: 10.766642568819663\n","step: 111800\n","learning_rate 5.455163806647073e-05\n","Step: 111800 -- Time: 18208 => ppl: 11.353572609713595\n","step: 111900\n","learning_rate 5.43884881355426e-05\n","Step: 111900 -- Time: 18238 => ppl: 10.753712471851642\n","step: 112000\n","learning_rate 5.422589884223241e-05\n","Step: 112000 -- Time: 18267 => ppl: 10.343690020557265\n","\n","1.1715801  1.7871122\n","\n","step: 112100\n","learning_rate 5.406386797807464e-05\n","Step: 112100 -- Time: 18296 => ppl: 10.464996307816898\n","step: 112200\n","learning_rate 5.390239334453429e-05\n","Step: 112200 -- Time: 18326 => ppl: 10.330888987749875\n","step: 112300\n","learning_rate 5.374147275295621e-05\n","Step: 112300 -- Time: 18355 => ppl: 10.955564978614445\n","step: 112400\n","learning_rate 5.358110402451469e-05\n","Step: 112400 -- Time: 18385 => ppl: 10.34286864874036\n","step: 112500\n","learning_rate 5.34212849901634e-05\n","Step: 112500 -- Time: 18414 => ppl: 10.561498131323596\n","step: 112600\n","learning_rate 5.326201349058554e-05\n","Step: 112600 -- Time: 18442 => ppl: 10.414847938887664\n","step: 112700\n","learning_rate 5.310328737614432e-05\n","Step: 112700 -- Time: 18470 => ppl: 10.376554896620181\n","step: 112800\n","learning_rate 5.294510450683373e-05\n","Step: 112800 -- Time: 18500 => ppl: 10.284815204058283\n","step: 112900\n","learning_rate 5.2787462752229576e-05\n","Step: 112900 -- Time: 18529 => ppl: 9.718193418862313\n","step: 113000\n","learning_rate 5.2630359991440794e-05\n","Step: 113000 -- Time: 18557 => ppl: 10.834085412116995\n","\n","1.5065441  2.4745593\n","\n","step: 113100\n","learning_rate 5.247379411306105e-05\n","Step: 113100 -- Time: 18587 => ppl: 10.560079281504745\n","step: 113200\n","learning_rate 5.231776301512064e-05\n","Step: 113200 -- Time: 18616 => ppl: 10.054457798022483\n","step: 113300\n","learning_rate 5.21622646050386e-05\n","Step: 113300 -- Time: 18644 => ppl: 9.77956145000634\n","step: 113400\n","learning_rate 5.200729679957517e-05\n","Step: 113400 -- Time: 18672 => ppl: 10.53815774882203\n","step: 113500\n","learning_rate 5.1852857524784475e-05\n","Step: 113500 -- Time: 18700 => ppl: 9.856485707872734\n","step: 113600\n","learning_rate 5.169894471596749e-05\n","Step: 113600 -- Time: 18728 => ppl: 10.148595898653443\n","step: 113700\n","learning_rate 5.154555631762527e-05\n","Step: 113700 -- Time: 18756 => ppl: 9.508940402720103\n","step: 113800\n","learning_rate 5.1392690283412467e-05\n","Step: 113800 -- Time: 18785 => ppl: 9.94226817986767\n","step: 113900\n","learning_rate 5.124034457609109e-05\n","Step: 113900 -- Time: 18813 => ppl: 10.212424439927032\n","step: 114000\n","learning_rate 5.1088517167484526e-05\n","Step: 114000 -- Time: 18843 => ppl: 10.036837557505258\n","\n","0.91543734  2.7594619\n","\n","step: 114100\n","learning_rate 5.093720603843185e-05\n","Step: 114100 -- Time: 18871 => ppl: 10.203988429803653\n","step: 114200\n","learning_rate 5.078640917874235e-05\n","Step: 114200 -- Time: 18898 => ppl: 9.816479017592242\n","step: 114300\n","learning_rate 5.0636124587150364e-05\n","Step: 114300 -- Time: 18926 => ppl: 10.152639889890125\n","step: 114400\n","learning_rate 5.048635027127032e-05\n","Step: 114400 -- Time: 18954 => ppl: 10.617009823143974\n","step: 114500\n","learning_rate 5.033708424755207e-05\n","Step: 114500 -- Time: 18981 => ppl: 9.891046798876477\n","step: 114600\n","learning_rate 5.0188324541236477e-05\n","Step: 114600 -- Time: 19009 => ppl: 9.780460917937528\n","step: 114700\n","learning_rate 5.00400691863112e-05\n","Step: 114700 -- Time: 19035 => ppl: 9.930561102225125\n","step: 114800\n","learning_rate 4.989231622546682e-05\n","Step: 114800 -- Time: 19064 => ppl: 10.362120555323749\n","step: 114900\n","learning_rate 4.974506371005312e-05\n","Step: 114900 -- Time: 19094 => ppl: 10.503030366656363\n","step: 115000\n","learning_rate 4.959830970003568e-05\n","Step: 115000 -- Time: 19120 => ppl: 9.293036774571759\n","validation\n","### Step: 115000 -- Time: 19167 => ppl: 10.528842917861889\n","cnt 147\n","\n","2.3172913  3.728835\n","\n","step: 115100\n","learning_rate 4.945205226395269e-05\n","Step: 115100 -- Time: 19199 => ppl: 10.477295378746579\n","step: 115200\n","learning_rate 4.9306289478872e-05\n","Step: 115200 -- Time: 19229 => ppl: 9.676704890064446\n","step: 115300\n","learning_rate 4.916101943034842e-05\n","Step: 115300 -- Time: 19256 => ppl: 10.13230571987806\n","step: 115400\n","learning_rate 4.9016240212381304e-05\n","Step: 115400 -- Time: 19285 => ppl: 9.852572714484287\n","step: 115500\n","learning_rate 4.887194992737227e-05\n","Step: 115500 -- Time: 19312 => ppl: 10.328951123187789\n","step: 115600\n","learning_rate 4.872814668608326e-05\n","Step: 115600 -- Time: 19341 => ppl: 9.868885294639059\n","step: 115700\n","learning_rate 4.8584828607594784e-05\n","Step: 115700 -- Time: 19370 => ppl: 9.722873485494995\n","step: 115800\n","learning_rate 4.8441993819264404e-05\n","Step: 115800 -- Time: 19399 => ppl: 10.38139253026841\n","step: 115900\n","learning_rate 4.829964045668548e-05\n","Step: 115900 -- Time: 19429 => ppl: 11.23216863039096\n","step: 116000\n","learning_rate 4.81577666636461e-05\n","Step: 116000 -- Time: 19459 => ppl: 10.1902135778587\n","\n","0.92122525  2.327992\n","\n","step: 116100\n","learning_rate 4.801637059208828e-05\n","Step: 116100 -- Time: 19489 => ppl: 11.019852653188073\n","step: 116200\n","learning_rate 4.787545040206738e-05\n","Step: 116200 -- Time: 19523 => ppl: 10.88221654989483\n","step: 116300\n","learning_rate 4.773500426171173e-05\n","Step: 116300 -- Time: 19552 => ppl: 11.210998188869198\n","step: 116400\n","learning_rate 4.759503034718252e-05\n","Step: 116400 -- Time: 19584 => ppl: 10.9074280053114\n","step: 116500\n","learning_rate 4.745552684263388e-05\n","Step: 116500 -- Time: 19615 => ppl: 10.812465364024122\n","step: 116600\n","learning_rate 4.7316491940173214e-05\n","Step: 116600 -- Time: 19647 => ppl: 11.016474006809293\n","step: 116700\n","learning_rate 4.717792383982169e-05\n","Step: 116700 -- Time: 19679 => ppl: 11.290090548942986\n","step: 116800\n","learning_rate 4.703982074947506e-05\n","Step: 116800 -- Time: 19710 => ppl: 10.999528758123475\n","step: 116900\n","learning_rate 4.690218088486459e-05\n","Step: 116900 -- Time: 19742 => ppl: 10.710053071412487\n","step: 117000\n","learning_rate 4.676500246951827e-05\n","Step: 117000 -- Time: 19774 => ppl: 11.803072392027289\n","\n","1.1040462  2.7544088\n","\n","step: 117100\n","learning_rate 4.662828373472226e-05\n","Step: 117100 -- Time: 19804 => ppl: 10.541074949284436\n","step: 117200\n","learning_rate 4.649202291948245e-05\n","Step: 117200 -- Time: 19836 => ppl: 10.955944494723743\n","step: 117300\n","learning_rate 4.635621827048634e-05\n","Step: 117300 -- Time: 19866 => ppl: 10.851309131912759\n","step: 117400\n","learning_rate 4.62208680420651e-05\n","Step: 117400 -- Time: 19897 => ppl: 10.98425231463257\n","step: 117500\n","learning_rate 4.608597049615581e-05\n","Step: 117500 -- Time: 19930 => ppl: 10.574925680985547\n","step: 117600\n","learning_rate 4.5951523902263925e-05\n","Step: 117600 -- Time: 19961 => ppl: 10.326282952779069\n","step: 117700\n","learning_rate 4.5817526537426004e-05\n","Step: 117700 -- Time: 19990 => ppl: 10.535705128100446\n","step: 117800\n","learning_rate 4.5683976686172554e-05\n","Step: 117800 -- Time: 20019 => ppl: 10.594766567061635\n","step: 117900\n","learning_rate 4.555087264049113e-05\n","Step: 117900 -- Time: 20048 => ppl: 10.324810166526587\n","step: 118000\n","learning_rate 4.541821269978964e-05\n","Step: 118000 -- Time: 20079 => ppl: 10.976239083981715\n","\n","1.2116944  2.319035\n","\n","step: 118100\n","learning_rate 4.528599517085985e-05\n","Step: 118100 -- Time: 20111 => ppl: 10.591024069505412\n","step: 118200\n","learning_rate 4.5154218367841084e-05\n","Step: 118200 -- Time: 20141 => ppl: 10.226781260758866\n","step: 118300\n","learning_rate 4.5022880612184136e-05\n","Step: 118300 -- Time: 20171 => ppl: 10.748798514629865\n","step: 118400\n","learning_rate 4.4891980232615354e-05\n","Step: 118400 -- Time: 20202 => ppl: 10.917834784916513\n","step: 118500\n","learning_rate 4.476151556510096e-05\n","Step: 118500 -- Time: 20232 => ppl: 9.962582177326034\n","step: 118600\n","learning_rate 4.4631484952811544e-05\n","Step: 118600 -- Time: 20263 => ppl: 10.965792671874778\n","step: 118700\n","learning_rate 4.450188674608675e-05\n","Step: 118700 -- Time: 20294 => ppl: 11.170859507394681\n","step: 118800\n","learning_rate 4.437271930240018e-05\n","Step: 118800 -- Time: 20325 => ppl: 10.582974516320743\n","step: 118900\n","learning_rate 4.4243980986324484e-05\n","Step: 118900 -- Time: 20355 => ppl: 10.011769363728535\n","step: 119000\n","learning_rate 4.41156701694966e-05\n","Step: 119000 -- Time: 20385 => ppl: 10.498062100477659\n","\n","1.498166  2.3006468\n","\n","step: 119100\n","learning_rate 4.3987785230583244e-05\n","Step: 119100 -- Time: 20414 => ppl: 9.85895616504145\n","step: 119200\n","learning_rate 4.386032455524658e-05\n","Step: 119200 -- Time: 20442 => ppl: 10.317110835722577\n","step: 119300\n","learning_rate 4.3733286536110034e-05\n","Step: 119300 -- Time: 20470 => ppl: 9.761859440098318\n","step: 119400\n","learning_rate 4.360666957272435e-05\n","Step: 119400 -- Time: 20499 => ppl: 9.76594952114648\n","step: 119500\n","learning_rate 4.348047207153379e-05\n","Step: 119500 -- Time: 20529 => ppl: 10.096535925292477\n","step: 119600\n","learning_rate 4.3354692445842566e-05\n","Step: 119600 -- Time: 20556 => ppl: 9.375159225219766\n","step: 119700\n","learning_rate 4.322932911578139e-05\n","Step: 119700 -- Time: 20585 => ppl: 9.254551964879896\n","step: 119800\n","learning_rate 4.3104380508274275e-05\n","Step: 119800 -- Time: 20613 => ppl: 9.508592240945275\n","step: 119900\n","learning_rate 4.297984505700548e-05\n","Step: 119900 -- Time: 20643 => ppl: 10.116310655608764\n","step: 120000\n","learning_rate 4.285572120238664e-05\n","Step: 120000 -- Time: 20671 => ppl: 9.678236722730729\n","validation\n","### Step: 120000 -- Time: 20718 => ppl: 10.436480069574491\n","cnt 148\n","\n","1.0516937  1.923069\n","\n","step: 120100\n","learning_rate 4.273200739152407e-05\n","Step: 120100 -- Time: 20754 => ppl: 10.168566167213747\n","step: 120200\n","learning_rate 4.260870207818628e-05\n","Step: 120200 -- Time: 20784 => ppl: 9.787407281060915\n","step: 120300\n","learning_rate 4.2485803722771636e-05\n","Step: 120300 -- Time: 20815 => ppl: 11.27084142614822\n","step: 120400\n","learning_rate 4.2363310792276186e-05\n","Step: 120400 -- Time: 20844 => ppl: 9.881031565504221\n","step: 120500\n","learning_rate 4.2241221760261715e-05\n","Step: 120500 -- Time: 20873 => ppl: 9.848620275847894\n","step: 120600\n","learning_rate 4.211953510682391e-05\n","Step: 120600 -- Time: 20904 => ppl: 10.87614764689178\n","step: 120700\n","learning_rate 4.1998249318560744e-05\n","Step: 120700 -- Time: 20934 => ppl: 9.896310929089886\n","step: 120800\n","learning_rate 4.187736288854101e-05\n","Step: 120800 -- Time: 20962 => ppl: 10.24389448485551\n","step: 120900\n","learning_rate 4.175687431627304e-05\n","Step: 120900 -- Time: 20991 => ppl: 9.602748840301274\n","step: 121000\n","learning_rate 4.1636782107673586e-05\n","Step: 121000 -- Time: 21020 => ppl: 10.21504310209105\n","\n","1.08922  1.7888927\n","\n","step: 121100\n","learning_rate 4.151708477503687e-05\n","Step: 121100 -- Time: 21050 => ppl: 10.27294595603908\n","step: 121200\n","learning_rate 4.1397780837003776e-05\n","Step: 121200 -- Time: 21079 => ppl: 10.006101070353855\n","step: 121300\n","learning_rate 4.127886881853129e-05\n","Step: 121300 -- Time: 21111 => ppl: 10.557412566768834\n","step: 121400\n","learning_rate 4.1160347250862016e-05\n","Step: 121400 -- Time: 21142 => ppl: 10.044551115143893\n","step: 121500\n","learning_rate 4.104221467149388e-05\n","Step: 121500 -- Time: 21172 => ppl: 10.145047211810024\n","step: 121600\n","learning_rate 4.092446962415006e-05\n","Step: 121600 -- Time: 21201 => ppl: 10.742949135356815\n","step: 121700\n","learning_rate 4.080711065874896e-05\n","Step: 121700 -- Time: 21232 => ppl: 10.039448824379038\n","step: 121800\n","learning_rate 4.0690136331374485e-05\n","Step: 121800 -- Time: 21262 => ppl: 10.88339104926813\n","step: 121900\n","learning_rate 4.057354520424636e-05\n","Step: 121900 -- Time: 21294 => ppl: 10.864841912331526\n","step: 122000\n","learning_rate 4.045733584569068e-05\n","Step: 122000 -- Time: 21326 => ppl: 11.022018182371472\n","\n","1.1956514  2.1104112\n","\n","step: 122100\n","learning_rate 4.034150683011058e-05\n","Step: 122100 -- Time: 21356 => ppl: 10.155725603752325\n","step: 122200\n","learning_rate 4.0226056737957103e-05\n","Step: 122200 -- Time: 21385 => ppl: 9.768045460048425\n","step: 122300\n","learning_rate 4.011098415570017e-05\n","Step: 122300 -- Time: 21415 => ppl: 10.268920602016799\n","step: 122400\n","learning_rate 3.999628767579977e-05\n","Step: 122400 -- Time: 21445 => ppl: 10.615084475404219\n","step: 122500\n","learning_rate 3.988196589667723e-05\n","Step: 122500 -- Time: 21475 => ppl: 10.344744263382614\n","step: 122600\n","learning_rate 3.976801742268672e-05\n","Step: 122600 -- Time: 21503 => ppl: 10.107334070429136\n","step: 122700\n","learning_rate 3.965444086408687e-05\n","Step: 122700 -- Time: 21531 => ppl: 10.373169218157097\n","step: 122800\n","learning_rate 3.9541234837012513e-05\n","Step: 122800 -- Time: 21562 => ppl: 9.96032808818206\n","step: 122900\n","learning_rate 3.942839796344664e-05\n","Step: 122900 -- Time: 21591 => ppl: 10.217662840985419\n","step: 123000\n","learning_rate 3.931592887119245e-05\n","Step: 123000 -- Time: 21621 => ppl: 10.036276956110447\n","\n","14.693706  22.38189\n","\n","step: 123100\n","learning_rate 3.920382619384561e-05\n","Step: 123100 -- Time: 21650 => ppl: 10.121973266356743\n","step: 123200\n","learning_rate 3.9092088570766584e-05\n","Step: 123200 -- Time: 21679 => ppl: 10.005259274583615\n","step: 123300\n","learning_rate 3.898071464705319e-05\n","Step: 123300 -- Time: 21710 => ppl: 10.78405278428568\n","step: 123400\n","learning_rate 3.886970307351325e-05\n","Step: 123400 -- Time: 21741 => ppl: 9.809748236584715\n","step: 123500\n","learning_rate 3.875905250663741e-05\n","Step: 123500 -- Time: 21770 => ppl: 9.445458830584853\n","step: 123600\n","learning_rate 3.864876160857211e-05\n","Step: 123600 -- Time: 21798 => ppl: 9.92479727162882\n","step: 123700\n","learning_rate 3.853882904709266e-05\n","Step: 123700 -- Time: 21827 => ppl: 10.280903475333275\n","step: 123800\n","learning_rate 3.842925349557652e-05\n","Step: 123800 -- Time: 21855 => ppl: 9.938474688360513\n","step: 123900\n","learning_rate 3.8320033632976686e-05\n","Step: 123900 -- Time: 21883 => ppl: 9.894854221091048\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P77M_mDTQGf8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}